
==> Audit <==
|---------|---------------------------|----------|-------------------------------|---------|---------------------|---------------------|
| Command |           Args            | Profile  |             User              | Version |     Start Time      |      End Time       |
|---------|---------------------------|----------|-------------------------------|---------|---------------------|---------------------|
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 08:56 BST | 03 Jun 25 08:57 BST |
| stop    |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 08:58 BST | 03 Jun 25 08:58 BST |
| delete  |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 08:58 BST | 03 Jun 25 08:58 BST |
| start   | --nodes 3 --driver=docker | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 08:59 BST | 03 Jun 25 09:01 BST |
| image   | load frontend:v1          | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:05 BST | 03 Jun 25 09:05 BST |
| image   | load catalogservice:v1    | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:06 BST | 03 Jun 25 09:06 BST |
| image   | load streamingservice:v1  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:06 BST | 03 Jun 25 09:06 BST |
| image   | load iploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:07 BST | 03 Jun 25 09:07 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:07 BST | 03 Jun 25 09:08 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:08 BST | 03 Jun 25 09:10 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:11 BST | 03 Jun 25 09:11 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:14 BST | 03 Jun 25 09:15 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:17 BST | 03 Jun 25 09:18 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:19 BST | 03 Jun 25 09:22 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:24 BST | 03 Jun 25 09:24 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:26 BST | 03 Jun 25 09:26 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:30 BST | 03 Jun 25 09:31 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:31 BST | 03 Jun 25 09:32 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:33 BST | 03 Jun 25 09:33 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:34 BST | 03 Jun 25 09:50 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:51 BST | 03 Jun 25 09:51 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:52 BST | 03 Jun 25 09:55 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:56 BST | 03 Jun 25 09:56 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 09:58 BST | 03 Jun 25 09:59 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 10:00 BST | 03 Jun 25 10:00 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 10:07 BST | 03 Jun 25 10:08 BST |
| start   |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:40 BST | 03 Jun 25 16:41 BST |
| image   | load frontend:v1          | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:44 BST | 03 Jun 25 16:44 BST |
| service | start                     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:46 BST |                     |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:46 BST | 03 Jun 25 16:49 BST |
| image   | load frontend:v1          | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:52 BST | 03 Jun 25 16:52 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:52 BST | 03 Jun 25 16:55 BST |
| stop    |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:55 BST | 03 Jun 25 16:56 BST |
| delete  |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:56 BST | 03 Jun 25 16:56 BST |
| start   | --nodes=3 --driver=docker | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 16:57 BST | 03 Jun 25 16:59 BST |
| image   | load frontend:v1          | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:00 BST | 03 Jun 25 17:00 BST |
| image   | load catalogservice:v1    | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:00 BST | 03 Jun 25 17:00 BST |
| image   | load streamingservice:v1  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:01 BST | 03 Jun 25 17:01 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:02 BST | 03 Jun 25 17:02 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:02 BST |                     |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:03 BST |                     |
| image   | load frontend:v1          | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:04 BST | 03 Jun 25 17:04 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:04 BST | 03 Jun 25 17:06 BST |
| image   | load frontend:v1          | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:06 BST | 03 Jun 25 17:06 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 03 Jun 25 17:07 BST | 03 Jun 25 17:08 BST |
| start   |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 15:46 BST |                     |
| delete  |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 15:46 BST | 04 Jun 25 15:46 BST |
| start   | -n 3 --driver=docker      | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 15:46 BST |                     |
| delete  |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:04 BST | 04 Jun 25 16:04 BST |
| start   | --nodes 3 --driver=docker | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:04 BST |                     |
| start   | --nodes=3 --driver=docker | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:08 BST | 04 Jun 25 16:09 BST |
| stop    |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:19 BST | 04 Jun 25 16:19 BST |
| delete  |                           | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:21 BST | 04 Jun 25 16:21 BST |
| start   | --nodes 3 --driver=docker | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:22 BST | 04 Jun 25 16:24 BST |
| image   | load frontend:v1          | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:28 BST | 04 Jun 25 16:28 BST |
| image   | load uploadservice:v1     | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:29 BST | 04 Jun 25 16:29 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:29 BST |                     |
| image   | load catalogservice:v1    | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:31 BST | 04 Jun 25 16:31 BST |
| image   | load streamingservice:v1  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:32 BST | 04 Jun 25 16:32 BST |
| service | frontend                  | minikube | USERPCD-TB5SH1O\Administrador | v1.36.0 | 04 Jun 25 16:32 BST |                     |
|---------|---------------------------|----------|-------------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/06/04 16:22:18
Running on machine: USERPCD-TB5SH1O
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0604 16:22:18.108649   15760 out.go:345] Setting OutFile to fd 100 ...
I0604 16:22:18.110468   15760 out.go:397] isatty.IsTerminal(100) = true
I0604 16:22:18.110468   15760 out.go:358] Setting ErrFile to fd 104...
I0604 16:22:18.110468   15760 out.go:397] isatty.IsTerminal(104) = true
I0604 16:22:18.121760   15760 out.go:352] Setting JSON to false
I0604 16:22:18.124823   15760 start.go:130] hostinfo: {"hostname":"USERPCD-TB5SH1O","uptime":8928,"bootTime":1749041609,"procs":306,"os":"windows","platform":"Microsoft Windows 11 Home","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.4061 Build 26100.4061","kernelVersion":"10.0.26100.4061 Build 26100.4061","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"bc4b97f1-5cb2-4079-b404-1dc863701f64"}
W0604 16:22:18.124823   15760 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0604 16:22:18.124823   15760 out.go:177] üòÑ  minikube v1.36.0 on Microsoft Windows 11 Home 10.0.26100.4061 Build 26100.4061
I0604 16:22:18.126335   15760 notify.go:220] Checking for updates...
I0604 16:22:18.126335   15760 driver.go:404] Setting default libvirt URI to qemu:///system
I0604 16:22:18.171239   15760 docker.go:123] docker version: linux-28.1.1:Docker Desktop 4.41.2 (191736)
I0604 16:22:18.175279   15760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0604 16:22:18.292726   15760 info.go:266] docker info: {ID:a16755b0-b782-4e5e-821e-19cffece579e Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:85 SystemTime:2025-06-04 15:22:18.281860317 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:24608333824 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0604 16:22:18.294067   15760 out.go:177] ‚ú®  Using the docker driver based on user configuration
I0604 16:22:18.294581   15760 start.go:304] selected driver: docker
I0604 16:22:18.294581   15760 start.go:908] validating driver "docker" against <nil>
I0604 16:22:18.294581   15760 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0604 16:22:18.302094   15760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0604 16:22:18.413899   15760 info.go:266] docker info: {ID:a16755b0-b782-4e5e-821e-19cffece579e Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:85 SystemTime:2025-06-04 15:22:18.402734371 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:24608333824 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0604 16:22:18.413899   15760 start_flags.go:311] no existing cluster config was found, will generate one from the flags 
I0604 16:22:18.444833   15760 start_flags.go:394] Using suggested 2700MB memory alloc based on sys=32581MB, container=23468MB
I0604 16:22:18.444833   15760 start_flags.go:958] Wait components to verify : map[apiserver:true system_pods:true]
I0604 16:22:18.445837   15760 out.go:177] üìå  Using Docker Desktop driver with root privileges
I0604 16:22:18.446340   15760 cni.go:84] Creating CNI manager for ""
I0604 16:22:18.446340   15760 cni.go:136] multinode detected (0 nodes found), recommending kindnet
I0604 16:22:18.446340   15760 start_flags.go:320] Found "CNI" CNI - setting NetworkPlugin=cni
I0604 16:22:18.446340   15760 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0604 16:22:18.446906   15760 out.go:177] üëç  Starting "minikube" primary control-plane node in "minikube" cluster
I0604 16:22:18.446906   15760 cache.go:121] Beginning downloading kic base image for docker with docker
I0604 16:22:18.447418   15760 out.go:177] üöú  Pulling base image v0.0.47 ...
I0604 16:22:18.447418   15760 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0604 16:22:18.447418   15760 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0604 16:22:18.447418   15760 preload.go:146] Found local preload: C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0604 16:22:18.447418   15760 cache.go:56] Caching tarball of preloaded images
I0604 16:22:18.448008   15760 preload.go:172] Found C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0604 16:22:18.448008   15760 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0604 16:22:18.448008   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:22:18.448008   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.minikube\profiles\minikube\config.json: {Name:mk4fab2aba3ff02f390621ed3f64a504763de31e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:18.535253   15760 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0604 16:22:18.536259   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:22:18.536259   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:22:18.536259   15760 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0604 16:22:18.536259   15760 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0604 16:22:18.536259   15760 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0604 16:22:18.536259   15760 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0604 16:22:18.536259   15760 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0604 16:22:18.536259   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:22:34.795022   15760 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0604 16:22:34.795022   15760 cache.go:230] Successfully downloaded all kic artifacts
I0604 16:22:34.795545   15760 start.go:360] acquireMachinesLock for minikube: {Name:mk39e63bf8d3c4641e7900b2d91b6416731ebc86 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0604 16:22:34.795545   15760 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0604 16:22:34.795545   15760 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0604 16:22:34.795545   15760 start.go:125] createHost starting for "" (driver="docker")
I0604 16:22:34.797135   15760 out.go:235] üî•  Creating docker container (CPUs=2, Memory=2700MB) ...
I0604 16:22:34.797649   15760 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0604 16:22:34.797649   15760 client.go:168] LocalClient.Create starting
I0604 16:22:34.797649   15760 main.go:141] libmachine: Reading certificate data from C:\Users\Administrador\.minikube\certs\ca.pem
I0604 16:22:34.797649   15760 main.go:141] libmachine: Decoding PEM data...
I0604 16:22:34.797649   15760 main.go:141] libmachine: Parsing certificate...
I0604 16:22:34.798599   15760 main.go:141] libmachine: Reading certificate data from C:\Users\Administrador\.minikube\certs\cert.pem
I0604 16:22:34.798599   15760 main.go:141] libmachine: Decoding PEM data...
I0604 16:22:34.798599   15760 main.go:141] libmachine: Parsing certificate...
I0604 16:22:34.814953   15760 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0604 16:22:34.855729   15760 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0604 16:22:34.859896   15760 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0604 16:22:34.859896   15760 cli_runner.go:164] Run: docker network inspect minikube
W0604 16:22:34.888971   15760 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0604 16:22:34.888971   15760 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0604 16:22:34.888971   15760 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0604 16:22:34.892968   15760 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0604 16:22:34.975886   15760 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001f9cd50}
I0604 16:22:34.976575   15760 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0604 16:22:34.980131   15760 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0604 16:22:35.031052   15760 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0604 16:22:35.031052   15760 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0604 16:22:35.040865   15760 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0604 16:22:35.074541   15760 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0604 16:22:35.106272   15760 oci.go:103] Successfully created a docker volume minikube
I0604 16:22:35.110288   15760 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0604 16:22:35.852558   15760 oci.go:107] Successfully prepared a docker volume minikube
I0604 16:22:35.852558   15760 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0604 16:22:35.852558   15760 kic.go:194] Starting extracting preloaded images to volume ...
I0604 16:22:35.856609   15760 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0604 16:22:39.597512   15760 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (3.7409034s)
I0604 16:22:39.597512   15760 kic.go:203] duration metric: took 3.7449541s to extract preloaded images to volume ...
I0604 16:22:39.601603   15760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0604 16:22:39.725470   15760 info.go:266] docker info: {ID:a16755b0-b782-4e5e-821e-19cffece579e Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:72 OomKillDisable:true NGoroutines:87 SystemTime:2025-06-04 15:22:39.713219515 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:24608333824 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0604 16:22:39.729582   15760 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0604 16:22:39.841603   15760 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=2700mb --memory-swap=2700mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0604 16:22:40.086777   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0604 16:22:40.124611   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:22:40.163975   15760 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0604 16:22:40.225802   15760 oci.go:144] the created container "minikube" has a running status.
I0604 16:22:40.225802   15760 kic.go:225] Creating ssh key for kic: C:\Users\Administrador\.minikube\machines\minikube\id_rsa...
I0604 16:22:40.409913   15760 kic_runner.go:191] docker (temp): C:\Users\Administrador\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0604 16:22:40.470937   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:22:40.516167   15760 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0604 16:22:40.517171   15760 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0604 16:22:40.581365   15760 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Administrador\.minikube\machines\minikube\id_rsa...
I0604 16:22:40.838340   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:22:40.872139   15760 machine.go:93] provisionDockerMachine start ...
I0604 16:22:40.875978   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:40.909989   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:22:40.915710   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52196 <nil> <nil>}
I0604 16:22:40.915710   15760 main.go:141] libmachine: About to run SSH command:
hostname
I0604 16:22:41.048624   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0604 16:22:41.048624   15760 ubuntu.go:169] provisioning hostname "minikube"
I0604 16:22:41.052146   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:41.085402   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:22:41.085402   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52196 <nil> <nil>}
I0604 16:22:41.085402   15760 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0604 16:22:41.210199   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0604 16:22:41.216212   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:41.252870   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:22:41.252870   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52196 <nil> <nil>}
I0604 16:22:41.252870   15760 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0604 16:22:41.372151   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0604 16:22:41.372151   15760 ubuntu.go:175] set auth options {CertDir:C:\Users\Administrador\.minikube CaCertPath:C:\Users\Administrador\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Administrador\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Administrador\.minikube\machines\server.pem ServerKeyPath:C:\Users\Administrador\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Administrador\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Administrador\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Administrador\.minikube}
I0604 16:22:41.372151   15760 ubuntu.go:177] setting up certificates
I0604 16:22:41.372151   15760 provision.go:84] configureAuth start
I0604 16:22:41.377553   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0604 16:22:41.408505   15760 provision.go:143] copyHostCerts
I0604 16:22:41.408505   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/ca.pem, removing ...
I0604 16:22:41.408505   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\ca.pem
I0604 16:22:41.408505   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\ca.pem --> C:\Users\Administrador\.minikube/ca.pem (1099 bytes)
I0604 16:22:41.409863   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/cert.pem, removing ...
I0604 16:22:41.409863   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\cert.pem
I0604 16:22:41.409863   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\cert.pem --> C:\Users\Administrador\.minikube/cert.pem (1139 bytes)
I0604 16:22:41.410456   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/key.pem, removing ...
I0604 16:22:41.410456   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\key.pem
I0604 16:22:41.410456   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\key.pem --> C:\Users\Administrador\.minikube/key.pem (1675 bytes)
I0604 16:22:41.411061   15760 provision.go:117] generating server cert: C:\Users\Administrador\.minikube\machines\server.pem ca-key=C:\Users\Administrador\.minikube\certs\ca.pem private-key=C:\Users\Administrador\.minikube\certs\ca-key.pem org=Administrador.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0604 16:22:41.540142   15760 provision.go:177] copyRemoteCerts
I0604 16:22:41.541734   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0604 16:22:41.545973   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:41.579547   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:22:41.660485   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0604 16:22:41.677152   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\machines\server.pem --> /etc/docker/server.pem (1200 bytes)
I0604 16:22:41.690573   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0604 16:22:41.704247   15760 provision.go:87] duration metric: took 332.0964ms to configureAuth
I0604 16:22:41.704247   15760 ubuntu.go:193] setting minikube options for container-runtime
I0604 16:22:41.704760   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:22:41.708328   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:41.741444   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:22:41.741959   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52196 <nil> <nil>}
I0604 16:22:41.741959   15760 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0604 16:22:41.858457   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0604 16:22:41.858457   15760 ubuntu.go:71] root file system type: overlay
I0604 16:22:41.858457   15760 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0604 16:22:41.865643   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:41.901577   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:22:41.902087   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52196 <nil> <nil>}
I0604 16:22:41.902087   15760 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0604 16:22:42.025188   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0604 16:22:42.030032   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:42.062984   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:22:42.063488   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52196 <nil> <nil>}
I0604 16:22:42.063488   15760 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0604 16:22:44.285718   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-06-04 15:22:42.010035487 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0604 16:22:44.285718   15760 machine.go:96] duration metric: took 3.4135786s to provisionDockerMachine
I0604 16:22:44.285718   15760 client.go:171] duration metric: took 9.4880687s to LocalClient.Create
I0604 16:22:44.285718   15760 start.go:167] duration metric: took 9.4880687s to libmachine.API.Create "minikube"
I0604 16:22:44.285718   15760 start.go:293] postStartSetup for "minikube" (driver="docker")
I0604 16:22:44.285718   15760 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0604 16:22:44.287273   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0604 16:22:44.291446   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:44.323542   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:22:44.428115   15760 ssh_runner.go:195] Run: cat /etc/os-release
I0604 16:22:44.430663   15760 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0604 16:22:44.430663   15760 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0604 16:22:44.430663   15760 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0604 16:22:44.430663   15760 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0604 16:22:44.430663   15760 filesync.go:126] Scanning C:\Users\Administrador\.minikube\addons for local assets ...
I0604 16:22:44.430663   15760 filesync.go:126] Scanning C:\Users\Administrador\.minikube\files for local assets ...
I0604 16:22:44.431292   15760 start.go:296] duration metric: took 145.5746ms for postStartSetup
I0604 16:22:44.435494   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0604 16:22:44.463814   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:22:44.470682   15760 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0604 16:22:44.474247   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:44.505057   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:22:44.606726   15760 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0604 16:22:44.609611   15760 start.go:128] duration metric: took 9.8140663s to createHost
I0604 16:22:44.609611   15760 start.go:83] releasing machines lock for "minikube", held for 9.8140663s
I0604 16:22:44.613821   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0604 16:22:44.644956   15760 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0604 16:22:44.649574   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:44.649574   15760 ssh_runner.go:195] Run: cat /version.json
I0604 16:22:44.653443   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:44.680768   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:22:44.681848   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
W0604 16:22:44.767142   15760 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0604 16:22:44.785176   15760 ssh_runner.go:195] Run: systemctl --version
I0604 16:22:44.794569   15760 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0604 16:22:44.799513   15760 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0604 16:22:44.806036   15760 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0604 16:22:44.807208   15760 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0604 16:22:44.823536   15760 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0604 16:22:44.823536   15760 start.go:495] detecting cgroup driver to use...
I0604 16:22:44.823536   15760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0604 16:22:44.823536   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0604 16:22:44.838365   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0604 16:22:44.852069   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0604 16:22:44.858316   15760 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0604 16:22:44.863818   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0604 16:22:44.876891   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W0604 16:22:44.886771   15760 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0604 16:22:44.886771   15760 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0604 16:22:44.890410   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0604 16:22:44.903040   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0604 16:22:44.916133   15760 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0604 16:22:44.928609   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0604 16:22:44.940745   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0604 16:22:44.952589   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0604 16:22:44.960625   15760 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0604 16:22:44.966905   15760 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0604 16:22:44.972647   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:22:45.041323   15760 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0604 16:22:45.122190   15760 start.go:495] detecting cgroup driver to use...
I0604 16:22:45.122190   15760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0604 16:22:45.124118   15760 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0604 16:22:45.131036   15760 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0604 16:22:45.133303   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0604 16:22:45.140170   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0604 16:22:45.156308   15760 ssh_runner.go:195] Run: which cri-dockerd
I0604 16:22:45.162314   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0604 16:22:45.168615   15760 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0604 16:22:45.180953   15760 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0604 16:22:45.261120   15760 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0604 16:22:45.334980   15760 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0604 16:22:45.334980   15760 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0604 16:22:45.346766   15760 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0604 16:22:45.354297   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:22:45.434307   15760 ssh_runner.go:195] Run: sudo systemctl restart docker
I0604 16:22:48.510942   15760 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.0751198s)
I0604 16:22:48.512045   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0604 16:22:48.520597   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0604 16:22:48.528635   15760 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0604 16:22:48.612169   15760 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0604 16:22:48.696037   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:22:48.774836   15760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0604 16:22:48.783637   15760 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0604 16:22:48.791181   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:22:48.875028   15760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0604 16:22:48.935515   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0604 16:22:48.942674   15760 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0604 16:22:48.949099   15760 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0604 16:22:48.951692   15760 start.go:563] Will wait 60s for crictl version
I0604 16:22:48.957597   15760 ssh_runner.go:195] Run: which crictl
I0604 16:22:48.962286   15760 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0604 16:22:49.028896   15760 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0604 16:22:49.032466   15760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0604 16:22:49.090193   15760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0604 16:22:49.104177   15760 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0604 16:22:49.107766   15760 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0604 16:22:49.192168   15760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0604 16:22:49.199078   15760 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0604 16:22:49.201717   15760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0604 16:22:49.211893   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0604 16:22:49.247916   15760 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0604 16:22:49.247916   15760 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0604 16:22:49.251614   15760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0604 16:22:49.264738   15760 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0604 16:22:49.264738   15760 docker.go:632] Images already preloaded, skipping extraction
I0604 16:22:49.268461   15760 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0604 16:22:49.280594   15760 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0604 16:22:49.280594   15760 cache_images.go:84] Images are preloaded, skipping loading
I0604 16:22:49.281222   15760 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.33.1 docker true true} ...
I0604 16:22:49.281222   15760 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0604 16:22:49.285569   15760 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0604 16:22:49.407800   15760 cni.go:84] Creating CNI manager for ""
I0604 16:22:49.407800   15760 cni.go:136] multinode detected (1 nodes found), recommending kindnet
I0604 16:22:49.407800   15760 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0604 16:22:49.407800   15760 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0604 16:22:49.407800   15760 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0604 16:22:49.410586   15760 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0604 16:22:49.416351   15760 binaries.go:44] Found k8s binaries, skipping transfer
I0604 16:22:49.417505   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0604 16:22:49.422822   15760 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0604 16:22:49.432997   15760 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0604 16:22:49.443409   15760 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0604 16:22:49.460208   15760 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0604 16:22:49.462825   15760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0604 16:22:49.471169   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:22:49.540913   15760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0604 16:22:49.548903   15760 certs.go:68] Setting up C:\Users\Administrador\.minikube\profiles\minikube for IP: 192.168.49.2
I0604 16:22:49.548903   15760 certs.go:194] generating shared ca certs ...
I0604 16:22:49.548903   15760 certs.go:226] acquiring lock for ca certs: {Name:mk0ae898084c6af2bbdc17114ee285626e52551a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:49.549420   15760 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Administrador\.minikube\ca.key
I0604 16:22:49.549420   15760 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Administrador\.minikube\proxy-client-ca.key
I0604 16:22:49.549420   15760 certs.go:256] generating profile certs ...
I0604 16:22:49.549420   15760 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\Administrador\.minikube\profiles\minikube\client.key
I0604 16:22:49.549420   15760 crypto.go:68] Generating cert C:\Users\Administrador\.minikube\profiles\minikube\client.crt with IP's: []
I0604 16:22:49.647246   15760 crypto.go:156] Writing cert to C:\Users\Administrador\.minikube\profiles\minikube\client.crt ...
I0604 16:22:49.647246   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.minikube\profiles\minikube\client.crt: {Name:mk0b15702e2dda7cecc058cdbd0d9b527a1a3721 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:49.647246   15760 crypto.go:164] Writing key to C:\Users\Administrador\.minikube\profiles\minikube\client.key ...
I0604 16:22:49.647246   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.minikube\profiles\minikube\client.key: {Name:mk5aa125325299b9e4da65ccf042c84e6a3da664 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:49.648773   15760 certs.go:363] generating signed profile cert for "minikube": C:\Users\Administrador\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0604 16:22:49.648773   15760 crypto.go:68] Generating cert C:\Users\Administrador\.minikube\profiles\minikube\apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0604 16:22:49.676355   15760 crypto.go:156] Writing cert to C:\Users\Administrador\.minikube\profiles\minikube\apiserver.crt.7fb57e3c ...
I0604 16:22:49.676355   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.minikube\profiles\minikube\apiserver.crt.7fb57e3c: {Name:mkaeac60fce85ff9bf4e3f9f2dccc0fccb18e7c8 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:49.677359   15760 crypto.go:164] Writing key to C:\Users\Administrador\.minikube\profiles\minikube\apiserver.key.7fb57e3c ...
I0604 16:22:49.677359   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.minikube\profiles\minikube\apiserver.key.7fb57e3c: {Name:mka1f6ba5b5638e2cbdc072b24efc57e509a4b24 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:49.677863   15760 certs.go:381] copying C:\Users\Administrador\.minikube\profiles\minikube\apiserver.crt.7fb57e3c -> C:\Users\Administrador\.minikube\profiles\minikube\apiserver.crt
I0604 16:22:49.683992   15760 certs.go:385] copying C:\Users\Administrador\.minikube\profiles\minikube\apiserver.key.7fb57e3c -> C:\Users\Administrador\.minikube\profiles\minikube\apiserver.key
I0604 16:22:49.683992   15760 certs.go:363] generating signed profile cert for "aggregator": C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.key
I0604 16:22:49.683992   15760 crypto.go:68] Generating cert C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0604 16:22:49.812563   15760 crypto.go:156] Writing cert to C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.crt ...
I0604 16:22:49.812563   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.crt: {Name:mk33179992c1db32278c08f29024426527d278ee Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:49.813229   15760 crypto.go:164] Writing key to C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.key ...
I0604 16:22:49.813229   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.key: {Name:mk2431d8845c2db35a83224f03ba4475a78afd94 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:49.821398   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\ca-key.pem (1675 bytes)
I0604 16:22:49.821398   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\ca.pem (1099 bytes)
I0604 16:22:49.821398   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\cert.pem (1139 bytes)
I0604 16:22:49.821398   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\key.pem (1675 bytes)
I0604 16:22:49.821398   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0604 16:22:49.837208   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0604 16:22:49.851428   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0604 16:22:49.866000   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0604 16:22:49.879124   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0604 16:22:49.893050   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0604 16:22:49.908346   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0604 16:22:49.921884   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0604 16:22:49.937297   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0604 16:22:49.951299   15760 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0604 16:22:49.968383   15760 ssh_runner.go:195] Run: openssl version
I0604 16:22:49.975181   15760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0604 16:22:49.987787   15760 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0604 16:22:49.990503   15760 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 27 11:16 /usr/share/ca-certificates/minikubeCA.pem
I0604 16:22:49.996716   15760 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0604 16:22:50.002607   15760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0604 16:22:50.013612   15760 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0604 16:22:50.017180   15760 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0604 16:22:50.017180   15760 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0604 16:22:50.021827   15760 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0604 16:22:50.035564   15760 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0604 16:22:50.043553   15760 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0604 16:22:50.049862   15760 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0604 16:22:50.051057   15760 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0604 16:22:50.057207   15760 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0604 16:22:50.057207   15760 kubeadm.go:157] found existing configuration files:

I0604 16:22:50.058771   15760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0604 16:22:50.064508   15760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0604 16:22:50.065613   15760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0604 16:22:50.073103   15760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0604 16:22:50.078256   15760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0604 16:22:50.079995   15760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0604 16:22:50.086101   15760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0604 16:22:50.091597   15760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0604 16:22:50.093121   15760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0604 16:22:50.099631   15760 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0604 16:22:50.105040   15760 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0604 16:22:50.106585   15760 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0604 16:22:50.111509   15760 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0604 16:22:50.161068   15760 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0604 16:22:50.164200   15760 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0604 16:22:50.210621   15760 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0604 16:22:57.327764   15760 kubeadm.go:310] [init] Using Kubernetes version: v1.33.1
I0604 16:22:57.328283   15760 kubeadm.go:310] [preflight] Running pre-flight checks
I0604 16:22:57.328283   15760 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0604 16:22:57.328283   15760 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0604 16:22:57.328283   15760 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0604 16:22:57.328283   15760 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0604 16:22:57.328899   15760 out.go:235]     ‚ñ™ Generating certificates and keys ...
I0604 16:22:57.328899   15760 kubeadm.go:310] [certs] Using existing ca certificate authority
I0604 16:22:57.328899   15760 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0604 16:22:57.328899   15760 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0604 16:22:57.328899   15760 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0604 16:22:57.329417   15760 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0604 16:22:57.329417   15760 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0604 16:22:57.329417   15760 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0604 16:22:57.329417   15760 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0604 16:22:57.329417   15760 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0604 16:22:57.329417   15760 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0604 16:22:57.329417   15760 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0604 16:22:57.329929   15760 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0604 16:22:57.329929   15760 kubeadm.go:310] [certs] Generating "sa" key and public key
I0604 16:22:57.329929   15760 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0604 16:22:57.329929   15760 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0604 16:22:57.329929   15760 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0604 16:22:57.329929   15760 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0604 16:22:57.329929   15760 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0604 16:22:57.329929   15760 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0604 16:22:57.329929   15760 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0604 16:22:57.330488   15760 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0604 16:22:57.330488   15760 out.go:235]     ‚ñ™ Booting up control plane ...
I0604 16:22:57.330488   15760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0604 16:22:57.330488   15760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0604 16:22:57.330488   15760 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0604 16:22:57.331101   15760 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0604 16:22:57.331101   15760 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0604 16:22:57.331101   15760 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0604 16:22:57.331101   15760 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0604 16:22:57.331101   15760 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0604 16:22:57.331101   15760 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.370261ms
I0604 16:22:57.331622   15760 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0604 16:22:57.331622   15760 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0604 16:22:57.331622   15760 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0604 16:22:57.331622   15760 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0604 16:22:57.331622   15760 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 2.028516745s
I0604 16:22:57.331622   15760 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 2.227930645s
I0604 16:22:57.332131   15760 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 3.501606648s
I0604 16:22:57.332131   15760 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0604 16:22:57.332131   15760 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0604 16:22:57.332131   15760 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0604 16:22:57.332131   15760 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0604 16:22:57.332645   15760 kubeadm.go:310] [bootstrap-token] Using token: p8iriy.jothxl999e4loe53
I0604 16:22:57.332645   15760 out.go:235]     ‚ñ™ Configuring RBAC rules ...
I0604 16:22:57.332645   15760 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0604 16:22:57.332645   15760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0604 16:22:57.333241   15760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0604 16:22:57.333241   15760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0604 16:22:57.333241   15760 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0604 16:22:57.333241   15760 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0604 16:22:57.333755   15760 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0604 16:22:57.333755   15760 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0604 16:22:57.333755   15760 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0604 16:22:57.333755   15760 kubeadm.go:310] 
I0604 16:22:57.333755   15760 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0604 16:22:57.333755   15760 kubeadm.go:310] 
I0604 16:22:57.333755   15760 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0604 16:22:57.333755   15760 kubeadm.go:310] 
I0604 16:22:57.333755   15760 kubeadm.go:310]   mkdir -p $HOME/.kube
I0604 16:22:57.333755   15760 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0604 16:22:57.333755   15760 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0604 16:22:57.333755   15760 kubeadm.go:310] 
I0604 16:22:57.333755   15760 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0604 16:22:57.333755   15760 kubeadm.go:310] 
I0604 16:22:57.334305   15760 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0604 16:22:57.334305   15760 kubeadm.go:310] 
I0604 16:22:57.334305   15760 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0604 16:22:57.334305   15760 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0604 16:22:57.334305   15760 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0604 16:22:57.334305   15760 kubeadm.go:310] 
I0604 16:22:57.334305   15760 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0604 16:22:57.334305   15760 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0604 16:22:57.334305   15760 kubeadm.go:310] 
I0604 16:22:57.334305   15760 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token p8iriy.jothxl999e4loe53 \
I0604 16:22:57.334817   15760 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:02fc0006667a53d3f271a00aff3b02b8ef9e5003f83a6cc79334b0f4c0b2063b \
I0604 16:22:57.334817   15760 kubeadm.go:310] 	--control-plane 
I0604 16:22:57.334817   15760 kubeadm.go:310] 
I0604 16:22:57.334817   15760 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0604 16:22:57.334817   15760 kubeadm.go:310] 
I0604 16:22:57.334817   15760 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token p8iriy.jothxl999e4loe53 \
I0604 16:22:57.334817   15760 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:02fc0006667a53d3f271a00aff3b02b8ef9e5003f83a6cc79334b0f4c0b2063b 
I0604 16:22:57.334817   15760 cni.go:84] Creating CNI manager for ""
I0604 16:22:57.334817   15760 cni.go:136] multinode detected (1 nodes found), recommending kindnet
I0604 16:22:57.335336   15760 out.go:177] üîó  Configuring CNI (Container Networking Interface) ...
I0604 16:22:57.343104   15760 ssh_runner.go:195] Run: stat /opt/cni/bin/portmap
I0604 16:22:57.346668   15760 cni.go:182] applying CNI manifest using /var/lib/minikube/binaries/v1.33.1/kubectl ...
I0604 16:22:57.346668   15760 ssh_runner.go:362] scp memory --> /var/tmp/minikube/cni.yaml (2601 bytes)
I0604 16:22:57.359273   15760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl apply --kubeconfig=/var/lib/minikube/kubeconfig -f /var/tmp/minikube/cni.yaml
I0604 16:22:57.484531   15760 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0604 16:22:57.486089   15760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_06_04T16_22_57_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0604 16:22:57.486608   15760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0604 16:22:57.490237   15760 ops.go:34] apiserver oom_adj: -16
I0604 16:22:57.535229   15760 kubeadm.go:1105] duration metric: took 50.6983ms to wait for elevateKubeSystemPrivileges
I0604 16:22:57.535229   15760 kubeadm.go:394] duration metric: took 7.5180488s to StartCluster
I0604 16:22:57.535229   15760 settings.go:142] acquiring lock: {Name:mkf1ca437943f631878d308aa8f07e616d157530 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:57.535229   15760 settings.go:150] Updating kubeconfig:  C:\Users\Administrador\.kube\config
I0604 16:22:57.535737   15760 lock.go:35] WriteFile acquiring C:\Users\Administrador\.kube\config: {Name:mk7522798478e22dfe03163710c5d2ec592972cf Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:22:57.536258   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I0604 16:22:57.536258   15760 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0604 16:22:57.536258   15760 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0604 16:22:57.536258   15760 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0604 16:22:57.536258   15760 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0604 16:22:57.536258   15760 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0604 16:22:57.536258   15760 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0604 16:22:57.536258   15760 host.go:66] Checking if "minikube" exists ...
I0604 16:22:57.536258   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:22:57.536258   15760 out.go:177] üîé  Verifying Kubernetes components...
I0604 16:22:57.538425   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:22:57.546401   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:22:57.547407   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:22:57.586773   15760 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0604 16:22:57.586773   15760 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0604 16:22:57.586773   15760 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0604 16:22:57.588314   15760 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0604 16:22:57.588314   15760 host.go:66] Checking if "minikube" exists ...
I0604 16:22:57.588314   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.65.254 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I0604 16:22:57.591795   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:57.598733   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:22:57.627326   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:22:57.629973   15760 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0604 16:22:57.629973   15760 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0604 16:22:57.634064   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:22:57.658444   15760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0604 16:22:57.673528   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:22:57.744679   15760 start.go:971] {"host.minikube.internal": 192.168.65.254} host record injected into CoreDNS's ConfigMap
I0604 16:22:57.751090   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0604 16:22:57.752765   15760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0604 16:22:57.794269   15760 api_server.go:52] waiting for apiserver process to appear ...
I0604 16:22:57.795804   15760 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0604 16:22:57.858134   15760 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0604 16:22:58.005197   15760 api_server.go:72] duration metric: took 468.9388ms to wait for apiserver process to appear ...
I0604 16:22:58.005197   15760 api_server.go:88] waiting for apiserver healthz status ...
I0604 16:22:58.005197   15760 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:52200/healthz ...
I0604 16:22:58.012920   15760 api_server.go:279] https://127.0.0.1:52200/healthz returned 200:
ok
I0604 16:22:58.013433   15760 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I0604 16:22:58.014439   15760 addons.go:514] duration metric: took 478.1808ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0604 16:22:58.014439   15760 api_server.go:141] control plane version: v1.33.1
I0604 16:22:58.014439   15760 api_server.go:131] duration metric: took 9.242ms to wait for apiserver health ...
I0604 16:22:58.014439   15760 system_pods.go:43] waiting for kube-system pods to appear ...
I0604 16:22:58.016453   15760 system_pods.go:59] 5 kube-system pods found
I0604 16:22:58.016453   15760 system_pods.go:61] "etcd-minikube" [6f428a46-72d0-4571-b4e9-aca0c4eeba13] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0604 16:22:58.016453   15760 system_pods.go:61] "kube-apiserver-minikube" [31e4571e-b3e1-4994-9223-d857c87c9dbc] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0604 16:22:58.016453   15760 system_pods.go:61] "kube-controller-manager-minikube" [ae9aa0ed-4b57-4053-9376-f7c328f04426] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0604 16:22:58.016453   15760 system_pods.go:61] "kube-scheduler-minikube" [43273eba-4617-4131-94c7-6e4a860e3334] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0604 16:22:58.016453   15760 system_pods.go:61] "storage-provisioner" [1c52890b-0da2-4a54-90fe-7df174da04fd] Pending
I0604 16:22:58.016453   15760 system_pods.go:74] duration metric: took 2.0139ms to wait for pod list to return data ...
I0604 16:22:58.016453   15760 kubeadm.go:578] duration metric: took 480.1947ms to wait for: map[apiserver:true system_pods:true]
I0604 16:22:58.016453   15760 node_conditions.go:102] verifying NodePressure condition ...
I0604 16:22:58.019506   15760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0604 16:22:58.019506   15760 node_conditions.go:123] node cpu capacity is 16
I0604 16:22:58.019506   15760 node_conditions.go:105] duration metric: took 3.0527ms to run NodePressure ...
I0604 16:22:58.019506   15760 start.go:241] waiting for startup goroutines ...
I0604 16:22:58.247077   15760 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I0604 16:22:58.247077   15760 start.go:246] waiting for cluster config update ...
I0604 16:22:58.247077   15760 start.go:255] writing updated cluster config ...
I0604 16:22:58.248591   15760 out.go:201] 
I0604 16:22:58.250676   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:22:58.250676   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:22:58.252405   15760 out.go:177] üëç  Starting "minikube-m02" worker node in "minikube" cluster
I0604 16:22:58.256422   15760 cache.go:121] Beginning downloading kic base image for docker with docker
I0604 16:22:58.257424   15760 out.go:177] üöú  Pulling base image v0.0.47 ...
I0604 16:22:58.257973   15760 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0604 16:22:58.257973   15760 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0604 16:22:58.257973   15760 cache.go:56] Caching tarball of preloaded images
I0604 16:22:58.258498   15760 preload.go:172] Found C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0604 16:22:58.258498   15760 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0604 16:22:58.258498   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:22:58.353492   15760 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0604 16:22:58.353492   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:22:58.353492   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:22:58.353492   15760 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0604 16:22:58.354011   15760 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0604 16:22:58.354011   15760 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0604 16:22:58.354011   15760 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0604 16:22:58.354011   15760 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0604 16:22:58.354011   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:23:15.107269   15760 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0604 16:23:15.107269   15760 cache.go:230] Successfully downloaded all kic artifacts
I0604 16:23:15.107269   15760 start.go:360] acquireMachinesLock for minikube-m02: {Name:mkc23c6e0a1e64f627613c3ea7d14fb22ef46391 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0604 16:23:15.107269   15760 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube-m02"
I0604 16:23:15.107269   15760 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name:m02 IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}
I0604 16:23:15.108392   15760 start.go:125] createHost starting for "m02" (driver="docker")
I0604 16:23:15.108838   15760 out.go:235] üî•  Creating docker container (CPUs=2, Memory=2700MB) ...
I0604 16:23:15.109853   15760 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0604 16:23:15.109853   15760 client.go:168] LocalClient.Create starting
I0604 16:23:15.110372   15760 main.go:141] libmachine: Reading certificate data from C:\Users\Administrador\.minikube\certs\ca.pem
I0604 16:23:15.110372   15760 main.go:141] libmachine: Decoding PEM data...
I0604 16:23:15.110372   15760 main.go:141] libmachine: Parsing certificate...
I0604 16:23:15.110372   15760 main.go:141] libmachine: Reading certificate data from C:\Users\Administrador\.minikube\certs\cert.pem
I0604 16:23:15.110372   15760 main.go:141] libmachine: Decoding PEM data...
I0604 16:23:15.110372   15760 main.go:141] libmachine: Parsing certificate...
I0604 16:23:15.127379   15760 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0604 16:23:15.169226   15760 network_create.go:77] Found existing network {name:minikube subnet:0xc0016575f0 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0604 16:23:15.169226   15760 kic.go:121] calculated static IP "192.168.49.3" for the "minikube-m02" container
I0604 16:23:15.177014   15760 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0604 16:23:15.215452   15760 cli_runner.go:164] Run: docker volume create minikube-m02 --label name.minikube.sigs.k8s.io=minikube-m02 --label created_by.minikube.sigs.k8s.io=true
I0604 16:23:15.252948   15760 oci.go:103] Successfully created a docker volume minikube-m02
I0604 16:23:15.257153   15760 cli_runner.go:164] Run: docker run --rm --name minikube-m02-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --entrypoint /usr/bin/test -v minikube-m02:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0604 16:23:15.836235   15760 oci.go:107] Successfully prepared a docker volume minikube-m02
I0604 16:23:15.836235   15760 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0604 16:23:15.836235   15760 kic.go:194] Starting extracting preloaded images to volume ...
I0604 16:23:15.842573   15760 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0604 16:23:19.422733   15760 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m02:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (3.5801601s)
I0604 16:23:19.422733   15760 kic.go:203] duration metric: took 3.5864987s to extract preloaded images to volume ...
I0604 16:23:19.426819   15760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0604 16:23:19.556329   15760 info.go:266] docker info: {ID:a16755b0-b782-4e5e-821e-19cffece579e Containers:1 ContainersRunning:1 ContainersPaused:0 ContainersStopped:0 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:86 OomKillDisable:true NGoroutines:99 SystemTime:2025-06-04 15:23:19.546090686 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:24608333824 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0604 16:23:19.561224   15760 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0604 16:23:19.676679   15760 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m02 --name minikube-m02 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m02 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m02 --network minikube --ip 192.168.49.3 --volume minikube-m02:/var --security-opt apparmor=unconfined --memory=2700mb --memory-swap=2700mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0604 16:23:19.961384   15760 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Running}}
I0604 16:23:20.001917   15760 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0604 16:23:20.037228   15760 cli_runner.go:164] Run: docker exec minikube-m02 stat /var/lib/dpkg/alternatives/iptables
I0604 16:23:20.097764   15760 oci.go:144] the created container "minikube-m02" has a running status.
I0604 16:23:20.097764   15760 kic.go:225] Creating ssh key for kic: C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa...
I0604 16:23:20.202365   15760 kic_runner.go:191] docker (temp): C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0604 16:23:20.264405   15760 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0604 16:23:20.312484   15760 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0604 16:23:20.312484   15760 kic_runner.go:114] Args: [docker exec --privileged minikube-m02 chown docker:docker /home/docker/.ssh/authorized_keys]
I0604 16:23:20.377031   15760 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa...
I0604 16:23:20.626825   15760 cli_runner.go:164] Run: docker container inspect minikube-m02 --format={{.State.Status}}
I0604 16:23:20.655557   15760 machine.go:93] provisionDockerMachine start ...
I0604 16:23:20.659982   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:20.693473   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:20.699945   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52241 <nil> <nil>}
I0604 16:23:20.699945   15760 main.go:141] libmachine: About to run SSH command:
hostname
I0604 16:23:20.813759   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0604 16:23:20.813759   15760 ubuntu.go:169] provisioning hostname "minikube-m02"
I0604 16:23:20.817418   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:20.845812   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:20.845812   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52241 <nil> <nil>}
I0604 16:23:20.845812   15760 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m02 && echo "minikube-m02" | sudo tee /etc/hostname
I0604 16:23:20.980615   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m02

I0604 16:23:20.985450   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:21.014700   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:21.014700   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52241 <nil> <nil>}
I0604 16:23:21.014700   15760 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m02' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m02/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m02' | sudo tee -a /etc/hosts; 
			fi
		fi
I0604 16:23:21.135081   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0604 16:23:21.135081   15760 ubuntu.go:175] set auth options {CertDir:C:\Users\Administrador\.minikube CaCertPath:C:\Users\Administrador\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Administrador\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Administrador\.minikube\machines\server.pem ServerKeyPath:C:\Users\Administrador\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Administrador\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Administrador\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Administrador\.minikube}
I0604 16:23:21.135081   15760 ubuntu.go:177] setting up certificates
I0604 16:23:21.135081   15760 provision.go:84] configureAuth start
I0604 16:23:21.140173   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0604 16:23:21.172753   15760 provision.go:143] copyHostCerts
I0604 16:23:21.172753   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/ca.pem, removing ...
I0604 16:23:21.172753   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\ca.pem
I0604 16:23:21.173838   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\ca.pem --> C:\Users\Administrador\.minikube/ca.pem (1099 bytes)
I0604 16:23:21.173838   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/cert.pem, removing ...
I0604 16:23:21.173838   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\cert.pem
I0604 16:23:21.174360   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\cert.pem --> C:\Users\Administrador\.minikube/cert.pem (1139 bytes)
I0604 16:23:21.174360   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/key.pem, removing ...
I0604 16:23:21.174360   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\key.pem
I0604 16:23:21.174360   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\key.pem --> C:\Users\Administrador\.minikube/key.pem (1675 bytes)
I0604 16:23:21.174871   15760 provision.go:117] generating server cert: C:\Users\Administrador\.minikube\machines\server.pem ca-key=C:\Users\Administrador\.minikube\certs\ca.pem private-key=C:\Users\Administrador\.minikube\certs\ca-key.pem org=Administrador.minikube-m02 san=[127.0.0.1 192.168.49.3 localhost minikube minikube-m02]
I0604 16:23:21.298833   15760 provision.go:177] copyRemoteCerts
I0604 16:23:21.300365   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0604 16:23:21.303476   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:21.332886   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52241 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0604 16:23:21.416574   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0604 16:23:21.430740   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\machines\server.pem --> /etc/docker/server.pem (1220 bytes)
I0604 16:23:21.444269   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0604 16:23:21.459394   15760 provision.go:87] duration metric: took 324.3134ms to configureAuth
I0604 16:23:21.459394   15760 ubuntu.go:193] setting minikube options for container-runtime
I0604 16:23:21.459394   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:23:21.463329   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:21.495720   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:21.495720   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52241 <nil> <nil>}
I0604 16:23:21.495720   15760 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0604 16:23:21.624659   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0604 16:23:21.624659   15760 ubuntu.go:71] root file system type: overlay
I0604 16:23:21.624659   15760 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0604 16:23:21.629734   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:21.664040   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:21.664040   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52241 <nil> <nil>}
I0604 16:23:21.664040   15760 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0604 16:23:21.792461   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0604 16:23:21.796514   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:21.826933   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:21.826933   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52241 <nil> <nil>}
I0604 16:23:21.826933   15760 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0604 16:23:24.364118   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-06-04 15:23:21.790335267 +0000
@@ -1,46 +1,50 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Environment=NO_PROXY=192.168.49.2
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0604 16:23:24.364118   15760 machine.go:96] duration metric: took 3.7085608s to provisionDockerMachine
I0604 16:23:24.364118   15760 client.go:171] duration metric: took 9.2542641s to LocalClient.Create
I0604 16:23:24.364118   15760 start.go:167] duration metric: took 9.2542641s to libmachine.API.Create "minikube"
I0604 16:23:24.364118   15760 start.go:293] postStartSetup for "minikube-m02" (driver="docker")
I0604 16:23:24.364118   15760 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0604 16:23:24.367037   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0604 16:23:24.371316   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:24.404046   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52241 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0604 16:23:24.503704   15760 ssh_runner.go:195] Run: cat /etc/os-release
I0604 16:23:24.506334   15760 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0604 16:23:24.506334   15760 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0604 16:23:24.506334   15760 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0604 16:23:24.506334   15760 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0604 16:23:24.506334   15760 filesync.go:126] Scanning C:\Users\Administrador\.minikube\addons for local assets ...
I0604 16:23:24.506334   15760 filesync.go:126] Scanning C:\Users\Administrador\.minikube\files for local assets ...
I0604 16:23:24.507301   15760 start.go:296] duration metric: took 142.2162ms for postStartSetup
I0604 16:23:24.512274   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0604 16:23:24.543041   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:23:24.549857   15760 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0604 16:23:24.553584   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:24.585347   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52241 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0604 16:23:24.693009   15760 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0604 16:23:24.697346   15760 start.go:128] duration metric: took 9.5889542s to createHost
I0604 16:23:24.697346   15760 start.go:83] releasing machines lock for "minikube-m02", held for 9.5900774s
I0604 16:23:24.701331   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m02
I0604 16:23:24.732633   15760 out.go:177] üåê  Found network options:
I0604 16:23:24.733153   15760 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2
W0604 16:23:24.733663   15760 proxy.go:120] fail to check proxy env: Error ip not in block
I0604 16:23:24.733663   15760 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2
W0604 16:23:24.734181   15760 proxy.go:120] fail to check proxy env: Error ip not in block
W0604 16:23:24.734181   15760 proxy.go:120] fail to check proxy env: Error ip not in block
I0604 16:23:24.736224   15760 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0604 16:23:24.739792   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:24.741186   15760 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0604 16:23:24.745596   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m02
I0604 16:23:24.774065   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52241 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa Username:docker}
I0604 16:23:24.777975   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52241 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m02\id_rsa Username:docker}
W0604 16:23:24.852875   15760 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0604 16:23:24.865964   15760 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0604 16:23:24.871030   15760 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0604 16:23:24.872658   15760 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0604 16:23:24.886492   15760 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0604 16:23:24.886492   15760 start.go:495] detecting cgroup driver to use...
I0604 16:23:24.886492   15760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0604 16:23:24.886492   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0604 16:23:24.901780   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0604 16:23:24.913133   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0604 16:23:24.918593   15760 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0604 16:23:24.924038   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0604 16:23:24.935716   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0604 16:23:24.947384   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0604 16:23:24.958250   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W0604 16:23:24.966863   15760 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0604 16:23:24.966863   15760 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0604 16:23:24.969393   15760 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0604 16:23:24.981724   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0604 16:23:24.994802   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0604 16:23:25.006859   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0604 16:23:25.014882   15760 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0604 16:23:25.021506   15760 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0604 16:23:25.027263   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:25.099425   15760 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0604 16:23:25.201463   15760 start.go:495] detecting cgroup driver to use...
I0604 16:23:25.201463   15760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0604 16:23:25.202897   15760 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0604 16:23:25.210630   15760 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0604 16:23:25.212418   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0604 16:23:25.219403   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0604 16:23:25.234916   15760 ssh_runner.go:195] Run: which cri-dockerd
I0604 16:23:25.241089   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0604 16:23:25.248017   15760 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0604 16:23:25.260586   15760 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0604 16:23:25.338654   15760 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0604 16:23:25.373300   15760 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0604 16:23:25.373300   15760 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0604 16:23:25.386110   15760 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0604 16:23:25.394203   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:25.476955   15760 ssh_runner.go:195] Run: sudo systemctl restart docker
I0604 16:23:29.035741   15760 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.5587863s)
I0604 16:23:29.038785   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0604 16:23:29.046079   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0604 16:23:29.054652   15760 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0604 16:23:29.137374   15760 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0604 16:23:29.173799   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:29.273306   15760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0604 16:23:29.282851   15760 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0604 16:23:29.291254   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:29.368661   15760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0604 16:23:29.419010   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0604 16:23:29.426164   15760 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0604 16:23:29.432274   15760 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0604 16:23:29.434682   15760 start.go:563] Will wait 60s for crictl version
I0604 16:23:29.441232   15760 ssh_runner.go:195] Run: which crictl
I0604 16:23:29.444864   15760 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0604 16:23:29.468894   15760 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0604 16:23:29.472153   15760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0604 16:23:29.492129   15760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0604 16:23:29.506843   15760 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0604 16:23:29.506843   15760 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I0604 16:23:29.511199   15760 cli_runner.go:164] Run: docker exec -t minikube-m02 dig +short host.docker.internal
I0604 16:23:29.581880   15760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0604 16:23:29.588281   15760 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0604 16:23:29.590513   15760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0604 16:23:29.597612   15760 mustload.go:65] Loading cluster: minikube
I0604 16:23:29.598120   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:23:29.605887   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:23:29.637909   15760 host.go:66] Checking if "minikube" exists ...
I0604 16:23:29.642302   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0604 16:23:29.673616   15760 certs.go:68] Setting up C:\Users\Administrador\.minikube\profiles\minikube for IP: 192.168.49.3
I0604 16:23:29.673616   15760 certs.go:194] generating shared ca certs ...
I0604 16:23:29.674136   15760 certs.go:226] acquiring lock for ca certs: {Name:mk0ae898084c6af2bbdc17114ee285626e52551a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:23:29.674136   15760 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Administrador\.minikube\ca.key
I0604 16:23:29.674136   15760 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Administrador\.minikube\proxy-client-ca.key
I0604 16:23:29.674647   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\ca-key.pem (1675 bytes)
I0604 16:23:29.674647   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\ca.pem (1099 bytes)
I0604 16:23:29.674647   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\cert.pem (1139 bytes)
I0604 16:23:29.674647   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\key.pem (1675 bytes)
I0604 16:23:29.675162   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0604 16:23:29.689289   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0604 16:23:29.703432   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0604 16:23:29.716642   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0604 16:23:29.730354   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0604 16:23:29.749590   15760 ssh_runner.go:195] Run: openssl version
I0604 16:23:29.754494   15760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0604 16:23:29.766632   15760 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0604 16:23:29.768790   15760 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 27 11:16 /usr/share/ca-certificates/minikubeCA.pem
I0604 16:23:29.774734   15760 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0604 16:23:29.780533   15760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0604 16:23:29.791938   15760 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0604 16:23:29.794027   15760 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0604 16:23:29.794532   15760 kubeadm.go:926] updating node {m02 192.168.49.3 8443 v1.33.1 docker false true} ...
I0604 16:23:29.794532   15760 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m02 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.3

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0604 16:23:29.795555   15760 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0604 16:23:29.800832   15760 binaries.go:44] Found k8s binaries, skipping transfer
I0604 16:23:29.802479   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0604 16:23:29.806893   15760 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (311 bytes)
I0604 16:23:29.815394   15760 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0604 16:23:29.832238   15760 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0604 16:23:29.834316   15760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0604 16:23:29.841675   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:29.918152   15760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0604 16:23:29.925119   15760 host.go:66] Checking if "minikube" exists ...
I0604 16:23:29.926080   15760 start.go:317] joinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0604 16:23:29.926080   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm token create --print-join-command --ttl=0"
I0604 16:23:29.929559   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:23:29.962547   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:23:30.070814   15760 start.go:343] trying to join worker node "m02" to cluster: &{Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}
I0604 16:23:30.070814   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 6rmr6g.pz6r79btw1urzi7c --discovery-token-ca-cert-hash sha256:02fc0006667a53d3f271a00aff3b02b8ef9e5003f83a6cc79334b0f4c0b2063b --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m02"
I0604 16:23:30.766871   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0604 16:23:30.919426   15760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube-m02 minikube.k8s.io/updated_at=2025_06_04T16_23_30_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=false
I0604 16:23:30.968171   15760 start.go:319] duration metric: took 1.0420907s to joinCluster
I0604 16:23:30.968687   15760 start.go:235] Will wait 6m0s for node &{Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}
I0604 16:23:30.968687   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:23:30.968687   15760 out.go:177] üîé  Verifying Kubernetes components...
I0604 16:23:30.970874   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:31.039794   15760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0604 16:23:31.049767   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0604 16:23:31.081148   15760 kubeadm.go:578] duration metric: took 112.4604ms to wait for: map[apiserver:true system_pods:true]
I0604 16:23:31.081148   15760 node_conditions.go:102] verifying NodePressure condition ...
I0604 16:23:31.082689   15760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0604 16:23:31.082689   15760 node_conditions.go:123] node cpu capacity is 16
I0604 16:23:31.082689   15760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0604 16:23:31.082689   15760 node_conditions.go:123] node cpu capacity is 16
I0604 16:23:31.082689   15760 node_conditions.go:105] duration metric: took 1.5409ms to run NodePressure ...
I0604 16:23:31.082689   15760 start.go:241] waiting for startup goroutines ...
I0604 16:23:31.082689   15760 start.go:255] writing updated cluster config ...
I0604 16:23:31.083719   15760 out.go:201] 
I0604 16:23:31.088898   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:23:31.088898   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:23:31.090239   15760 out.go:177] üëç  Starting "minikube-m03" worker node in "minikube" cluster
I0604 16:23:31.090860   15760 cache.go:121] Beginning downloading kic base image for docker with docker
I0604 16:23:31.090860   15760 out.go:177] üöú  Pulling base image v0.0.47 ...
I0604 16:23:31.091377   15760 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0604 16:23:31.091377   15760 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0604 16:23:31.091377   15760 cache.go:56] Caching tarball of preloaded images
I0604 16:23:31.091377   15760 preload.go:172] Found C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0604 16:23:31.091377   15760 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0604 16:23:31.092381   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:23:31.181144   15760 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b to local cache
I0604 16:23:31.181653   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:23:31.181653   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:23:31.181653   15760 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory
I0604 16:23:31.181653   15760 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local cache directory, skipping pull
I0604 16:23:31.181653   15760 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in cache, skipping pull
I0604 16:23:31.181653   15760 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b as a tarball
I0604 16:23:31.181653   15760 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from local cache
I0604 16:23:31.181653   15760 localpath.go:146] windows sanitize: C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar -> C:\Users\Administrador\.minikube\cache\kic\amd64\kicbase_v0.0.47@sha256_6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b.tar
I0604 16:23:48.012342   15760 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b from cached tarball
I0604 16:23:48.012342   15760 cache.go:230] Successfully downloaded all kic artifacts
I0604 16:23:48.012877   15760 start.go:360] acquireMachinesLock for minikube-m03: {Name:mk5a4140d07718990b48097e0a342069b574f41f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0604 16:23:48.013388   15760 start.go:364] duration metric: took 511.1¬µs to acquireMachinesLock for "minikube-m03"
I0604 16:23:48.013388   15760 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name:m03 IP: Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}
I0604 16:23:48.013388   15760 start.go:125] createHost starting for "m03" (driver="docker")
I0604 16:23:48.014985   15760 out.go:235] üî•  Creating docker container (CPUs=2, Memory=2700MB) ...
I0604 16:23:48.016494   15760 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0604 16:23:48.016494   15760 client.go:168] LocalClient.Create starting
I0604 16:23:48.017000   15760 main.go:141] libmachine: Reading certificate data from C:\Users\Administrador\.minikube\certs\ca.pem
I0604 16:23:48.017000   15760 main.go:141] libmachine: Decoding PEM data...
I0604 16:23:48.017000   15760 main.go:141] libmachine: Parsing certificate...
I0604 16:23:48.017000   15760 main.go:141] libmachine: Reading certificate data from C:\Users\Administrador\.minikube\certs\cert.pem
I0604 16:23:48.018069   15760 main.go:141] libmachine: Decoding PEM data...
I0604 16:23:48.018069   15760 main.go:141] libmachine: Parsing certificate...
I0604 16:23:48.030050   15760 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0604 16:23:48.072972   15760 network_create.go:77] Found existing network {name:minikube subnet:0xc001f40300 gateway:[0 0 0 0 0 0 0 0 0 0 255 255 192 168 49 1] mtu:1500}
I0604 16:23:48.072972   15760 kic.go:121] calculated static IP "192.168.49.4" for the "minikube-m03" container
I0604 16:23:48.081281   15760 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0604 16:23:48.127354   15760 cli_runner.go:164] Run: docker volume create minikube-m03 --label name.minikube.sigs.k8s.io=minikube-m03 --label created_by.minikube.sigs.k8s.io=true
I0604 16:23:48.162721   15760 oci.go:103] Successfully created a docker volume minikube-m03
I0604 16:23:48.166918   15760 cli_runner.go:164] Run: docker run --rm --name minikube-m03-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --entrypoint /usr/bin/test -v minikube-m03:/var gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -d /var/lib
I0604 16:23:48.736564   15760 oci.go:107] Successfully prepared a docker volume minikube-m03
I0604 16:23:48.736564   15760 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0604 16:23:48.736564   15760 kic.go:194] Starting extracting preloaded images to volume ...
I0604 16:23:48.741002   15760 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir
I0604 16:23:52.401508   15760 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\Administrador\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube-m03:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b -I lz4 -xf /preloaded.tar -C /extractDir: (3.6605056s)
I0604 16:23:52.401508   15760 kic.go:203] duration metric: took 3.6649443s to extract preloaded images to volume ...
I0604 16:23:52.405636   15760 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0604 16:23:52.532764   15760 info.go:266] docker info: {ID:a16755b0-b782-4e5e-821e-19cffece579e Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:16 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:102 OomKillDisable:true NGoroutines:110 SystemTime:2025-06-04 15:23:52.523451929 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:20 KernelVersion:5.15.153.1-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:16 MemTotal:24608333824 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.1.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:05044ec0a9a75232cad458027ca83437aae3f4da Expected:} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.7] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.23.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:v0.3.0] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.35.1-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands Vendor:Docker Inc. Version:v0.1.8] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:mcp Path:C:\Program Files\Docker\cli-plugins\docker-mcp.exe SchemaVersion:0.1.0 ShortDescription:Docker MCP Plugin Vendor:Docker Inc. Version:dev] map[Name:model Path:C:\Program Files\Docker\cli-plugins\docker-model.exe SchemaVersion:0.1.0 ShortDescription:Docker Model Runner Vendor:Docker Inc. Version:v0.1.11] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.1]] Warnings:<nil>}}
I0604 16:23:52.537284   15760 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0604 16:23:52.667849   15760 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube-m03 --name minikube-m03 --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube-m03 --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube-m03 --network minikube --ip 192.168.49.4 --volume minikube-m03:/var --security-opt apparmor=unconfined --memory=2700mb --memory-swap=2700mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b
I0604 16:23:52.927647   15760 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Running}}
I0604 16:23:52.971004   15760 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0604 16:23:53.011708   15760 cli_runner.go:164] Run: docker exec minikube-m03 stat /var/lib/dpkg/alternatives/iptables
I0604 16:23:53.075970   15760 oci.go:144] the created container "minikube-m03" has a running status.
I0604 16:23:53.075970   15760 kic.go:225] Creating ssh key for kic: C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa...
I0604 16:23:53.350071   15760 kic_runner.go:191] docker (temp): C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0604 16:23:53.406553   15760 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0604 16:23:53.449164   15760 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0604 16:23:53.449164   15760 kic_runner.go:114] Args: [docker exec --privileged minikube-m03 chown docker:docker /home/docker/.ssh/authorized_keys]
I0604 16:23:53.506797   15760 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa...
I0604 16:23:53.753916   15760 cli_runner.go:164] Run: docker container inspect minikube-m03 --format={{.State.Status}}
I0604 16:23:53.784546   15760 machine.go:93] provisionDockerMachine start ...
I0604 16:23:53.788871   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:53.822247   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:53.829222   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52285 <nil> <nil>}
I0604 16:23:53.829222   15760 main.go:141] libmachine: About to run SSH command:
hostname
I0604 16:23:53.941114   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0604 16:23:53.941114   15760 ubuntu.go:169] provisioning hostname "minikube-m03"
I0604 16:23:53.945128   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:53.981072   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:53.981072   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52285 <nil> <nil>}
I0604 16:23:53.981072   15760 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube-m03 && echo "minikube-m03" | sudo tee /etc/hostname
I0604 16:23:54.108024   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube-m03

I0604 16:23:54.112014   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:54.145298   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:54.145298   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52285 <nil> <nil>}
I0604 16:23:54.145298   15760 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube-m03' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube-m03/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube-m03' | sudo tee -a /etc/hosts; 
			fi
		fi
I0604 16:23:54.271157   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0604 16:23:54.271157   15760 ubuntu.go:175] set auth options {CertDir:C:\Users\Administrador\.minikube CaCertPath:C:\Users\Administrador\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Administrador\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Administrador\.minikube\machines\server.pem ServerKeyPath:C:\Users\Administrador\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Administrador\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Administrador\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Administrador\.minikube}
I0604 16:23:54.271157   15760 ubuntu.go:177] setting up certificates
I0604 16:23:54.271157   15760 provision.go:84] configureAuth start
I0604 16:23:54.277898   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0604 16:23:54.311121   15760 provision.go:143] copyHostCerts
I0604 16:23:54.311121   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/ca.pem, removing ...
I0604 16:23:54.311121   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\ca.pem
I0604 16:23:54.311121   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\ca.pem --> C:\Users\Administrador\.minikube/ca.pem (1099 bytes)
I0604 16:23:54.311710   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/cert.pem, removing ...
I0604 16:23:54.311710   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\cert.pem
I0604 16:23:54.311710   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\cert.pem --> C:\Users\Administrador\.minikube/cert.pem (1139 bytes)
I0604 16:23:54.312231   15760 exec_runner.go:144] found C:\Users\Administrador\.minikube/key.pem, removing ...
I0604 16:23:54.312231   15760 exec_runner.go:203] rm: C:\Users\Administrador\.minikube\key.pem
I0604 16:23:54.312754   15760 exec_runner.go:151] cp: C:\Users\Administrador\.minikube\certs\key.pem --> C:\Users\Administrador\.minikube/key.pem (1675 bytes)
I0604 16:23:54.312754   15760 provision.go:117] generating server cert: C:\Users\Administrador\.minikube\machines\server.pem ca-key=C:\Users\Administrador\.minikube\certs\ca.pem private-key=C:\Users\Administrador\.minikube\certs\ca-key.pem org=Administrador.minikube-m03 san=[127.0.0.1 192.168.49.4 localhost minikube minikube-m03]
I0604 16:23:54.554626   15760 provision.go:177] copyRemoteCerts
I0604 16:23:54.556143   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0604 16:23:54.561058   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:54.598209   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52285 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0604 16:23:54.683965   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1099 bytes)
I0604 16:23:54.697218   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\machines\server.pem --> /etc/docker/server.pem (1220 bytes)
I0604 16:23:54.710213   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0604 16:23:54.723224   15760 provision.go:87] duration metric: took 452.067ms to configureAuth
I0604 16:23:54.723224   15760 ubuntu.go:193] setting minikube options for container-runtime
I0604 16:23:54.723224   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:23:54.728328   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:54.761789   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:54.761789   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52285 <nil> <nil>}
I0604 16:23:54.761789   15760 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0604 16:23:54.881399   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0604 16:23:54.881399   15760 ubuntu.go:71] root file system type: overlay
I0604 16:23:54.881399   15760 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0604 16:23:54.886397   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:54.919134   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:54.919642   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52285 <nil> <nil>}
I0604 16:23:54.919642   15760 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment="NO_PROXY=192.168.49.2"
Environment="NO_PROXY=192.168.49.2,192.168.49.3"


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0604 16:23:55.048029   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure

Environment=NO_PROXY=192.168.49.2
Environment=NO_PROXY=192.168.49.2,192.168.49.3


# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0604 16:23:55.051760   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:55.088294   15760 main.go:141] libmachine: Using SSH client type: native
I0604 16:23:55.088294   15760 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x94a9e0] 0x94d520 <nil>  [] 0s} 127.0.0.1 52285 <nil> <nil>}
I0604 16:23:55.088294   15760 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0604 16:23:57.647497   15760 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-04-18 09:50:48.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-06-04 15:23:55.028420209 +0000
@@ -1,46 +1,51 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
+Environment=NO_PROXY=192.168.49.2
+Environment=NO_PROXY=192.168.49.2,192.168.49.3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0604 16:23:57.647497   15760 machine.go:96] duration metric: took 3.8629508s to provisionDockerMachine
I0604 16:23:57.647497   15760 client.go:171] duration metric: took 9.6310024s to LocalClient.Create
I0604 16:23:57.647497   15760 start.go:167] duration metric: took 9.6310024s to libmachine.API.Create "minikube"
I0604 16:23:57.647497   15760 start.go:293] postStartSetup for "minikube-m03" (driver="docker")
I0604 16:23:57.647497   15760 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0604 16:23:57.649558   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0604 16:23:57.653509   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:57.686438   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52285 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0604 16:23:57.780182   15760 ssh_runner.go:195] Run: cat /etc/os-release
I0604 16:23:57.782363   15760 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0604 16:23:57.782363   15760 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0604 16:23:57.782363   15760 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0604 16:23:57.782363   15760 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0604 16:23:57.782363   15760 filesync.go:126] Scanning C:\Users\Administrador\.minikube\addons for local assets ...
I0604 16:23:57.782890   15760 filesync.go:126] Scanning C:\Users\Administrador\.minikube\files for local assets ...
I0604 16:23:57.782890   15760 start.go:296] duration metric: took 135.3928ms for postStartSetup
I0604 16:23:57.787470   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0604 16:23:57.819896   15760 profile.go:143] Saving config to C:\Users\Administrador\.minikube\profiles\minikube\config.json ...
I0604 16:23:57.828022   15760 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0604 16:23:57.831783   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:57.862217   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52285 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0604 16:23:57.947773   15760 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0604 16:23:57.949910   15760 start.go:128] duration metric: took 9.9365227s to createHost
I0604 16:23:57.949910   15760 start.go:83] releasing machines lock for "minikube-m03", held for 9.9365227s
I0604 16:23:57.954684   15760 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube-m03
I0604 16:23:57.988376   15760 out.go:177] üåê  Found network options:
I0604 16:23:57.989928   15760 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
W0604 16:23:57.992087   15760 proxy.go:120] fail to check proxy env: Error ip not in block
W0604 16:23:57.992087   15760 proxy.go:120] fail to check proxy env: Error ip not in block
I0604 16:23:57.993333   15760 out.go:177]     ‚ñ™ NO_PROXY=192.168.49.2,192.168.49.3
W0604 16:23:57.994937   15760 proxy.go:120] fail to check proxy env: Error ip not in block
W0604 16:23:57.994937   15760 proxy.go:120] fail to check proxy env: Error ip not in block
W0604 16:23:57.994937   15760 proxy.go:120] fail to check proxy env: Error ip not in block
W0604 16:23:57.994937   15760 proxy.go:120] fail to check proxy env: Error ip not in block
I0604 16:23:57.996981   15760 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0604 16:23:58.002609   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:58.004126   15760 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0604 16:23:58.008428   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube-m03
I0604 16:23:58.035992   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52285 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa Username:docker}
I0604 16:23:58.041785   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52285 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube-m03\id_rsa Username:docker}
W0604 16:23:58.120072   15760 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0604 16:23:58.132849   15760 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0604 16:23:58.137529   15760 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0604 16:23:58.139368   15760 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0604 16:23:58.165433   15760 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0604 16:23:58.165433   15760 start.go:495] detecting cgroup driver to use...
I0604 16:23:58.165433   15760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0604 16:23:58.165433   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0604 16:23:58.180998   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0604 16:23:58.193459   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0604 16:23:58.200044   15760 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0604 16:23:58.205762   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0604 16:23:58.217546   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
W0604 16:23:58.225198   15760 out.go:270] ‚ùó  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0604 16:23:58.225198   15760 out.go:270] üí°  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0604 16:23:58.230315   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0604 16:23:58.243744   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0604 16:23:58.257244   15760 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0604 16:23:58.270052   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0604 16:23:58.282422   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0604 16:23:58.294950   15760 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0604 16:23:58.302529   15760 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0604 16:23:58.309518   15760 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0604 16:23:58.315315   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:58.405768   15760 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0604 16:23:58.482302   15760 start.go:495] detecting cgroup driver to use...
I0604 16:23:58.482302   15760 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0604 16:23:58.484174   15760 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0604 16:23:58.491648   15760 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0604 16:23:58.493731   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0604 16:23:58.501153   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0604 16:23:58.517835   15760 ssh_runner.go:195] Run: which cri-dockerd
I0604 16:23:58.522429   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0604 16:23:58.528098   15760 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0604 16:23:58.539536   15760 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0604 16:23:58.625045   15760 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0604 16:23:58.657778   15760 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0604 16:23:58.657778   15760 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0604 16:23:58.670736   15760 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0604 16:23:58.678986   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:23:58.775625   15760 ssh_runner.go:195] Run: sudo systemctl restart docker
I0604 16:24:02.547244   15760 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.7716186s)
I0604 16:24:02.549673   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0604 16:24:02.559468   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0604 16:24:02.567878   15760 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0604 16:24:02.654794   15760 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0604 16:24:02.692896   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:24:02.779072   15760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0604 16:24:02.790377   15760 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0604 16:24:02.798629   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:24:02.884764   15760 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0604 16:24:02.930604   15760 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0604 16:24:02.937584   15760 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0604 16:24:02.943580   15760 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0604 16:24:02.946433   15760 start.go:563] Will wait 60s for crictl version
I0604 16:24:02.952249   15760 ssh_runner.go:195] Run: which crictl
I0604 16:24:02.955483   15760 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0604 16:24:02.979899   15760 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0604 16:24:02.983082   15760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0604 16:24:03.003389   15760 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0604 16:24:03.018461   15760 out.go:235] üê≥  Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0604 16:24:03.018987   15760 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2
I0604 16:24:03.018987   15760 out.go:177]     ‚ñ™ env NO_PROXY=192.168.49.2,192.168.49.3
I0604 16:24:03.023618   15760 cli_runner.go:164] Run: docker exec -t minikube-m03 dig +short host.docker.internal
I0604 16:24:03.086716   15760 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0604 16:24:03.093324   15760 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0604 16:24:03.096407   15760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0604 16:24:03.103165   15760 mustload.go:65] Loading cluster: minikube
I0604 16:24:03.103165   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:24:03.111967   15760 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0604 16:24:03.143886   15760 host.go:66] Checking if "minikube" exists ...
I0604 16:24:03.148602   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0604 16:24:03.178764   15760 certs.go:68] Setting up C:\Users\Administrador\.minikube\profiles\minikube for IP: 192.168.49.4
I0604 16:24:03.178764   15760 certs.go:194] generating shared ca certs ...
I0604 16:24:03.178764   15760 certs.go:226] acquiring lock for ca certs: {Name:mk0ae898084c6af2bbdc17114ee285626e52551a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0604 16:24:03.178764   15760 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Administrador\.minikube\ca.key
I0604 16:24:03.178764   15760 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Administrador\.minikube\proxy-client-ca.key
I0604 16:24:03.179281   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\ca-key.pem (1675 bytes)
I0604 16:24:03.179281   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\ca.pem (1099 bytes)
I0604 16:24:03.179281   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\cert.pem (1139 bytes)
I0604 16:24:03.179281   15760 certs.go:484] found cert: C:\Users\Administrador\.minikube\certs\key.pem (1675 bytes)
I0604 16:24:03.179281   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0604 16:24:03.192724   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0604 16:24:03.204211   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0604 16:24:03.217622   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0604 16:24:03.229896   15760 ssh_runner.go:362] scp C:\Users\Administrador\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0604 16:24:03.249652   15760 ssh_runner.go:195] Run: openssl version
I0604 16:24:03.255664   15760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0604 16:24:03.268521   15760 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0604 16:24:03.271216   15760 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May 27 11:16 /usr/share/ca-certificates/minikubeCA.pem
I0604 16:24:03.277456   15760 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0604 16:24:03.283386   15760 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0604 16:24:03.295217   15760 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0604 16:24:03.297388   15760 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0604 16:24:03.297905   15760 kubeadm.go:926] updating node {m03 192.168.49.4 8443 v1.33.1 docker false true} ...
I0604 16:24:03.297905   15760 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube-m03 --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.4

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0604 16:24:03.299412   15760 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0604 16:24:03.304494   15760 binaries.go:44] Found k8s binaries, skipping transfer
I0604 16:24:03.306370   15760 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system
I0604 16:24:03.311917   15760 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (311 bytes)
I0604 16:24:03.321833   15760 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0604 16:24:03.336949   15760 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0604 16:24:03.339726   15760 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0604 16:24:03.347160   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:24:03.417203   15760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0604 16:24:03.425242   15760 host.go:66] Checking if "minikube" exists ...
I0604 16:24:03.425242   15760 start.go:317] joinCluster: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2700 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true} {Name:m02 IP:192.168.49.3 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true} {Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}] Addons:map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:false efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:false storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:true ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Administrador:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0604 16:24:03.425242   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm token create --print-join-command --ttl=0"
I0604 16:24:03.429352   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0604 16:24:03.461635   15760 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:52196 SSHKeyPath:C:\Users\Administrador\.minikube\machines\minikube\id_rsa Username:docker}
I0604 16:24:03.575497   15760 start.go:343] trying to join worker node "m03" to cluster: &{Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}
I0604 16:24:03.575497   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.33.1:$PATH" kubeadm join control-plane.minikube.internal:8443 --token 0t1t99.jlouigqv6bu45oil --discovery-token-ca-cert-hash sha256:02fc0006667a53d3f271a00aff3b02b8ef9e5003f83a6cc79334b0f4c0b2063b --ignore-preflight-errors=all --cri-socket unix:///var/run/cri-dockerd.sock --node-name=minikube-m03"
I0604 16:24:04.262369   15760 ssh_runner.go:195] Run: /bin/bash -c "sudo systemctl daemon-reload && sudo systemctl enable kubelet && sudo systemctl start kubelet"
I0604 16:24:04.379395   15760 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.33.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube-m03 minikube.k8s.io/updated_at=2025_06_04T16_24_04_0700 minikube.k8s.io/version=v1.36.0 minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=false
I0604 16:24:04.426817   15760 start.go:319] duration metric: took 1.001575s to joinCluster
I0604 16:24:04.426817   15760 start.go:235] Will wait 6m0s for node &{Name:m03 IP:192.168.49.4 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:false Worker:true}
I0604 16:24:04.427337   15760 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0604 16:24:04.427337   15760 out.go:177] üîé  Verifying Kubernetes components...
I0604 16:24:04.429421   15760 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0604 16:24:04.505235   15760 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0604 16:24:04.515942   15760 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0604 16:24:04.549073   15760 kubeadm.go:578] duration metric: took 122.2553ms to wait for: map[apiserver:true system_pods:true]
I0604 16:24:04.549073   15760 node_conditions.go:102] verifying NodePressure condition ...
I0604 16:24:04.551383   15760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0604 16:24:04.551383   15760 node_conditions.go:123] node cpu capacity is 16
I0604 16:24:04.551383   15760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0604 16:24:04.551383   15760 node_conditions.go:123] node cpu capacity is 16
I0604 16:24:04.551383   15760 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0604 16:24:04.551383   15760 node_conditions.go:123] node cpu capacity is 16
I0604 16:24:04.551383   15760 node_conditions.go:105] duration metric: took 2.3103ms to run NodePressure ...
I0604 16:24:04.551383   15760 start.go:241] waiting for startup goroutines ...
I0604 16:24:04.551383   15760 start.go:255] writing updated cluster config ...
I0604 16:24:04.558705   15760 ssh_runner.go:195] Run: rm -f paused
I0604 16:24:04.616950   15760 start.go:607] kubectl: 1.32.2, cluster: 1.33.1 (minor skew: 1)
I0604 16:24:04.618468   15760 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 04 15:22:46 minikube dockerd[1160]: time="2025-06-04T15:22:46.890624234Z" level=info msg="Completed buildkit initialization"
Jun 04 15:22:46 minikube dockerd[1160]: time="2025-06-04T15:22:46.894706035Z" level=info msg="Daemon has completed initialization"
Jun 04 15:22:46 minikube dockerd[1160]: time="2025-06-04T15:22:46.894767098Z" level=info msg="API listen on /var/run/docker.sock"
Jun 04 15:22:46 minikube dockerd[1160]: time="2025-06-04T15:22:46.894801777Z" level=info msg="API listen on [::]:2376"
Jun 04 15:22:46 minikube dockerd[1160]: time="2025-06-04T15:22:46.895539898Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Jun 04 15:22:46 minikube dockerd[1160]: time="2025-06-04T15:22:46.895910792Z" level=info msg="Daemon shutdown complete"
Jun 04 15:22:46 minikube dockerd[1160]: time="2025-06-04T15:22:46.895938374Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Jun 04 15:22:46 minikube systemd[1]: docker.service: Deactivated successfully.
Jun 04 15:22:46 minikube systemd[1]: Stopped Docker Application Container Engine.
Jun 04 15:22:46 minikube systemd[1]: Starting Docker Application Container Engine...
Jun 04 15:22:46 minikube dockerd[1465]: time="2025-06-04T15:22:46.916978822Z" level=info msg="Starting up"
Jun 04 15:22:46 minikube dockerd[1465]: time="2025-06-04T15:22:46.917565400Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Jun 04 15:22:46 minikube dockerd[1465]: time="2025-06-04T15:22:46.925357829Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Jun 04 15:22:46 minikube dockerd[1465]: time="2025-06-04T15:22:46.929983466Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Jun 04 15:22:46 minikube dockerd[1465]: time="2025-06-04T15:22:46.936899351Z" level=info msg="Loading containers: start."
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.453967401Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count 03a7ea7ad5860f598162b7b9853fd3794f923a77459332a86727e2446119be7b], retrying...."
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.476703853Z" level=info msg="Loading containers: done."
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.482963725Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.482987836Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.482991347Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.482993673Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.483005094Z" level=info msg="Docker daemon" commit=01f442b containerd-snapshotter=false storage-driver=overlay2 version=28.1.1
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.483020727Z" level=info msg="Initializing buildkit"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.503036131Z" level=info msg="Completed buildkit initialization"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.508050221Z" level=info msg="Daemon has completed initialization"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.508103082Z" level=info msg="API listen on /var/run/docker.sock"
Jun 04 15:22:48 minikube dockerd[1465]: time="2025-06-04T15:22:48.508132006Z" level=info msg="API listen on [::]:2376"
Jun 04 15:22:48 minikube systemd[1]: Started Docker Application Container Engine.
Jun 04 15:22:48 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Start docker client with request timeout 0s"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Hairpin mode is set to hairpin-veth"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Loaded network plugin cni"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Docker cri networking managed by network plugin cni"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Setting cgroupDriver cgroupfs"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Jun 04 15:22:48 minikube cri-dockerd[1772]: time="2025-06-04T15:22:48Z" level=info msg="Start cri-dockerd grpc backend"
Jun 04 15:22:48 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Jun 04 15:22:52 minikube cri-dockerd[1772]: time="2025-06-04T15:22:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/eb6731cbd4e0fc9ef1152eebf08d98ec1d17614be0d0ee86a20f6527406b747a/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:22:52 minikube cri-dockerd[1772]: time="2025-06-04T15:22:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d5957e3b2907e69cfd07755bcd14a9496a472f3465b5e7f73783d87fe3a5dee3/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:22:52 minikube cri-dockerd[1772]: time="2025-06-04T15:22:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a65c332b71b0a086dbb1c9e20493ce3e85f3aa0ee24fba7a8d063e11a4085d17/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:22:52 minikube cri-dockerd[1772]: time="2025-06-04T15:22:52Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/de6f4a9c0d9fa08eb58e440dc4ca5956df110448a71c033c544cf2767d72c86b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:23:02 minikube cri-dockerd[1772]: time="2025-06-04T15:23:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/1998639ad05b3f89adc0c7da8f0e4e0346d09a010b648ae9fb479a1687bcf8d9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:23:02 minikube cri-dockerd[1772]: time="2025-06-04T15:23:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/228a83e43192f86224172c277abaf42b23eb1106fcc2755d94e56f8b26c802dc/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:23:02 minikube cri-dockerd[1772]: time="2025-06-04T15:23:02Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:23:02 minikube cri-dockerd[1772]: time="2025-06-04T15:23:02Z" level=error msg="Error adding pod kube-system/coredns-674b8bbfcf-45jnf to network {docker 420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4}:/proc/3082/ns/net:loopback:: plugin type=\"loopback\" failed (add): missing network name:"
Jun 04 15:23:03 minikube cri-dockerd[1772]: time="2025-06-04T15:23:03Z" level=error msg="Error deleting pod kube-system/coredns-674b8bbfcf-45jnf from network {docker 420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4}:/proc/3082/ns/net:loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Jun 04 15:23:03 minikube dockerd[1465]: time="2025-06-04T15:23:03.020853532Z" level=info msg="ignoring event" container=420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 04 15:23:03 minikube cri-dockerd[1772]: time="2025-06-04T15:23:03Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/23f2b08a99d3551c1ef4ccf121aaede55d5e331356d7114d1b8c32039ca1640f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:23:04 minikube cri-dockerd[1772]: time="2025-06-04T15:23:04Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-674b8bbfcf-45jnf_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\""
Jun 04 15:23:04 minikube cri-dockerd[1772]: time="2025-06-04T15:23:04Z" level=error msg="Error deleting pod kube-system/coredns-674b8bbfcf-45jnf from network {docker 420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4}::loopback:: plugin type=\"loopback\" failed (delete): missing network name"
Jun 04 15:23:07 minikube cri-dockerd[1772]: time="2025-06-04T15:23:07Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Jun 04 15:23:07 minikube cri-dockerd[1772]: time="2025-06-04T15:23:07Z" level=info msg="Stop pulling image docker.io/kindest/kindnetd:v20250512-df8de77b: Status: Downloaded newer image for kindest/kindnetd:v20250512-df8de77b"
Jun 04 15:23:19 minikube cri-dockerd[1772]: time="2025-06-04T15:23:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/92ab94d92c4615b3526d53b0a10035f23ff8c77fb57fef05b1e52466f8365630/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Jun 04 15:23:23 minikube dockerd[1465]: time="2025-06-04T15:23:23.800600617Z" level=info msg="ignoring event" container=4274e39c9aade159db6975faa389ead1097872aa4d70631ed9880a1aeca59e17 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 04 15:30:10 minikube cri-dockerd[1772]: time="2025-06-04T15:30:10Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ca2fec234310d2317cfba7ea8e077ac72dc845cf38c979a34f4223b38d2de360/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 04 15:30:21 minikube cri-dockerd[1772]: time="2025-06-04T15:30:21Z" level=info msg="Pulling image mongo:4.4: 83b651df5384: Extracting [================>                                  ]  47.91MB/141.4MB"
Jun 04 15:30:23 minikube cri-dockerd[1772]: time="2025-06-04T15:30:23Z" level=info msg="Stop pulling image mongo:4.4: Status: Downloaded newer image for mongo:4.4"


==> container status <==
CONTAINER           IMAGE                                                                                      CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
66122bc7ca06f       mongo@sha256:52c42cbab240b3c5b1748582cc13ef46d521ddacae002bbbda645cebed270ec0              2 minutes ago       Running             mongodb                   0                   ca2fec234310d       mongodb-2
c5ed6a956bc30       6e38f40d628db                                                                              9 minutes ago       Running             storage-provisioner       1                   1998639ad05b3       storage-provisioner
aa29867a2bbe7       1cf5f116067c6                                                                              9 minutes ago       Running             coredns                   0                   92ab94d92c461       coredns-674b8bbfcf-45jnf
98ad3b2d2a5ec       kindest/kindnetd@sha256:07a4b3fe0077a0ae606cc0a200fc25a28fa64dcc30b8d311b461089969449f9a   9 minutes ago       Running             kindnet-cni               0                   23f2b08a99d35       kindnet-c9zlh
f175eeace40e4       b79c189b052cd                                                                              10 minutes ago      Running             kube-proxy                0                   228a83e43192f       kube-proxy-2wrr6
4274e39c9aade       6e38f40d628db                                                                              10 minutes ago      Exited              storage-provisioner       0                   1998639ad05b3       storage-provisioner
ce3d43e71fbb4       c6ab243b29f82                                                                              10 minutes ago      Running             kube-apiserver            0                   de6f4a9c0d9fa       kube-apiserver-minikube
a9205ab0e395b       ef43894fa110c                                                                              10 minutes ago      Running             kube-controller-manager   0                   a65c332b71b0a       kube-controller-manager-minikube
558aef63aa436       398c985c0d950                                                                              10 minutes ago      Running             kube-scheduler            0                   d5957e3b2907e       kube-scheduler-minikube
ad3125d0fe953       499038711c081                                                                              10 minutes ago      Running             etcd                      0                   eb6731cbd4e0f       etcd-minikube


==> coredns [aa29867a2bbe] <==
[INFO] 10.244.1.3:54017 - 35816 "AAAA IN mongodb. udp 25 false 512" - - 0 4.001343838s
[ERROR] plugin/errors: 2 mongodb. AAAA: read udp 10.244.0.2:49250->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:54017 - 38647 "A IN mongodb. udp 25 false 512" - - 0 4.001340281s
[ERROR] plugin/errors: 2 mongodb. A: read udp 10.244.0.2:46932->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:49225 - 41719 "A IN mongodb. udp 25 false 512" - - 0 4.001384877s
[INFO] 10.244.1.3:49225 - 25334 "AAAA IN mongodb. udp 25 false 512" - - 0 4.001416708s
[ERROR] plugin/errors: 2 mongodb. A: read udp 10.244.0.2:49072->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mongodb. AAAA: read udp 10.244.0.2:56352->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:33637 - 16535 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000169682s
[INFO] 10.244.1.3:33637 - 43924 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000189212s
[INFO] 10.244.1.3:55065 - 55360 "AAAA IN mongodb.svc.cluster.local. udp 43 false 512" NXDOMAIN qr,aa,rd 136 0.000084492s
[INFO] 10.244.1.3:55065 - 12353 "A IN mongodb.svc.cluster.local. udp 43 false 512" NXDOMAIN qr,aa,rd 136 0.000092572s
[INFO] 10.244.1.3:37366 - 28079 "AAAA IN mongodb.cluster.local. udp 39 false 512" NXDOMAIN qr,aa,rd 132 0.000104412s
[INFO] 10.244.1.3:37366 - 62893 "A IN mongodb.cluster.local. udp 39 false 512" NXDOMAIN qr,aa,rd 132 0.000130093s
[INFO] 10.244.1.3:41080 - 46187 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000145397s
[INFO] 10.244.1.3:41080 - 4458 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000217732s
[INFO] 10.244.1.3:42660 - 49994 "AAAA IN mongodb.svc.cluster.local. udp 43 false 512" NXDOMAIN qr,aa,rd 136 0.000161766s
[INFO] 10.244.1.3:42660 - 15959 "A IN mongodb.svc.cluster.local. udp 43 false 512" NXDOMAIN qr,aa,rd 136 0.000137688s
[INFO] 10.244.1.3:36443 - 30204 "AAAA IN mongodb.cluster.local. udp 39 false 512" NXDOMAIN qr,aa,rd 132 0.00007647s
[INFO] 10.244.1.3:36443 - 48638 "A IN mongodb.cluster.local. udp 39 false 512" NXDOMAIN qr,aa,rd 132 0.000104681s
[INFO] 10.244.1.3:51532 - 37941 "AAAA IN mongodb. udp 25 false 512" - - 0 2.000511174s
[ERROR] plugin/errors: 2 mongodb. AAAA: read udp 10.244.0.2:46006->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:51532 - 18227 "A IN mongodb. udp 25 false 512" - - 0 2.00052897s
[ERROR] plugin/errors: 2 mongodb. A: read udp 10.244.0.2:38969->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:55056 - 35542 "AAAA IN mongodb. udp 25 false 512" - - 0 2.000442325s
[INFO] 10.244.1.3:55056 - 8407 "A IN mongodb. udp 25 false 512" - - 0 2.00041204s
[ERROR] plugin/errors: 2 mongodb. AAAA: read udp 10.244.0.2:40644->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mongodb. A: read udp 10.244.0.2:53339->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:51532 - 37941 "AAAA IN mongodb. udp 25 false 512" - - 0 2.000916538s
[ERROR] plugin/errors: 2 mongodb. AAAA: read udp 10.244.0.2:53616->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:51532 - 18227 "A IN mongodb. udp 25 false 512" - - 0 2.000924375s
[ERROR] plugin/errors: 2 mongodb. A: read udp 10.244.0.2:37767->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:55056 - 35542 "AAAA IN mongodb. udp 25 false 512" - - 0 2.000478699s
[INFO] 10.244.1.3:55056 - 8407 "A IN mongodb. udp 25 false 512" - - 0 2.000413838s
[ERROR] plugin/errors: 2 mongodb. AAAA: read udp 10.244.0.2:60356->192.168.65.254:53: i/o timeout
[ERROR] plugin/errors: 2 mongodb. A: read udp 10.244.0.2:50554->192.168.65.254:53: i/o timeout
[INFO] 10.244.1.3:36199 - 37144 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.00014277s
[INFO] 10.244.1.3:36199 - 38247 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 100 0.000182241s
[INFO] 10.244.1.3:53742 - 57276 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000126436s
[INFO] 10.244.1.3:53742 - 20930 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 100 0.000175707s
[INFO] 10.244.1.3:48508 - 54115 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000206402s
[INFO] 10.244.1.3:48508 - 32356 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 100 0.000271378s
[INFO] 10.244.1.3:56782 - 27321 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000132868s
[INFO] 10.244.1.3:56782 - 65215 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 100 0.000164331s
[INFO] 10.244.1.2:58073 - 60434 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000163738s
[INFO] 10.244.1.2:58073 - 44844 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.000222377s
[INFO] 10.244.1.2:49079 - 46189 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000129878s
[INFO] 10.244.1.2:49079 - 50542 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.000149456s
[INFO] 10.244.1.2:43152 - 4220 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000154195s
[INFO] 10.244.1.2:43152 - 19069 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.00020706s
[INFO] 10.244.1.2:54700 - 63543 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000129877s
[INFO] 10.244.1.2:54700 - 310 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.000152605s
[INFO] 10.244.2.2:56816 - 36424 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000199666s
[INFO] 10.244.2.2:56816 - 54601 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.000237569s
[INFO] 10.244.2.2:54658 - 21170 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000125032s
[INFO] 10.244.2.2:54658 - 43444 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.000181648s
[INFO] 10.244.2.2:38795 - 54061 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000130602s
[INFO] 10.244.2.2:38795 - 28974 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.000144384s
[INFO] 10.244.2.2:48294 - 21236 "AAAA IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 144 0.000150811s
[INFO] 10.244.2.2:48294 - 15867 "A IN mongodb.default.svc.cluster.local. udp 51 false 512" NOERROR qr,aa,rd 198 0.000189882s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_06_04T16_22_57_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 04 Jun 2025 15:22:54 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 04 Jun 2025 15:32:58 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 04 Jun 2025 15:32:29 +0000   Wed, 04 Jun 2025 15:22:53 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 04 Jun 2025 15:32:29 +0000   Wed, 04 Jun 2025 15:22:53 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 04 Jun 2025 15:32:29 +0000   Wed, 04 Jun 2025 15:22:53 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 04 Jun 2025 15:32:29 +0000   Wed, 04 Jun 2025 15:22:54 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24031576Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24031576Ki
  pods:               110
System Info:
  Machine ID:                 f46ca2837f084460bbded6486f156a72
  System UUID:                f46ca2837f084460bbded6486f156a72
  Boot ID:                    eb32f73a-3d0c-4284-9f48-f94d8ab88d17
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (9 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     mongodb-2                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         2m55s
  kube-system                 coredns-674b8bbfcf-45jnf            100m (0%)     0 (0%)      70Mi (0%)        170Mi (0%)     10m
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (0%)       0 (0%)         10m
  kube-system                 kindnet-c9zlh                       100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      10m
  kube-system                 kube-apiserver-minikube             250m (1%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-proxy-2wrr6                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (5%)   100m (0%)
  memory             220Mi (0%)  220Mi (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           10m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  10m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           10m                kubelet          Starting kubelet.
  Warning  CgroupV1                           10m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            10m (x8 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              10m (x8 over 10m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               10m (x7 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            10m                kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  10m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           10m                kubelet          Starting kubelet.
  Warning  CgroupV1                           10m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            10m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            10m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              10m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               10m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     10m                node-controller  Node minikube event: Registered Node minikube in Controller


Name:               minikube-m02
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m02
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=false
                    minikube.k8s.io/updated_at=2025_06_04T16_23_30_0700
                    minikube.k8s.io/version=v1.36.0
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 04 Jun 2025 15:23:30 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m02
  AcquireTime:     <unset>
  RenewTime:       Wed, 04 Jun 2025 15:33:00 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 04 Jun 2025 15:32:30 +0000   Wed, 04 Jun 2025 15:23:30 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 04 Jun 2025 15:32:30 +0000   Wed, 04 Jun 2025 15:23:30 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 04 Jun 2025 15:32:30 +0000   Wed, 04 Jun 2025 15:23:30 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 04 Jun 2025 15:32:30 +0000   Wed, 04 Jun 2025 15:23:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.3
  Hostname:    minikube-m02
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24031576Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24031576Ki
  pods:               110
System Info:
  Machine ID:                 d6e2623c0d7d456a8d00b0d63025fab3
  System UUID:                d6e2623c0d7d456a8d00b0d63025fab3
  Boot ID:                    eb32f73a-3d0c-4284-9f48-f94d8ab88d17
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.1.0/24
PodCIDRs:                     10.244.1.0/24
Non-terminated Pods:          (5 in total)
  Namespace                   Name                               CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                               ------------  ----------  ---------------  -------------  ---
  default                     catalogservice-85cb478d54-ztcrk    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m31s
  default                     mongodb-0                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m26s
  default                     uploadservice-6b9cd887cb-h7ft4     0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m30s
  kube-system                 kindnet-r7xrw                      100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      9m35s
  kube-system                 kube-proxy-tq5sg                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m35s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  100m (0%)
  memory             50Mi (0%)  50Mi (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                   Age                    From             Message
  ----     ------                   ----                   ----             -------
  Normal   Starting                 9m32s                  kube-proxy       
  Warning  CgroupV1                 9m35s                  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory  9m35s (x2 over 9m35s)  kubelet          Node minikube-m02 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    9m35s (x2 over 9m35s)  kubelet          Node minikube-m02 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     9m35s (x2 over 9m35s)  kubelet          Node minikube-m02 status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  9m35s                  kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           9m34s                  node-controller  Node minikube-m02 event: Registered Node minikube-m02 in Controller
  Normal   NodeReady                9m34s                  kubelet          Node minikube-m02 status is now: NodeReady


Name:               minikube-m03
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube-m03
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=false
                    minikube.k8s.io/updated_at=2025_06_04T16_24_04_0700
                    minikube.k8s.io/version=v1.36.0
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 04 Jun 2025 15:24:04 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube-m03
  AcquireTime:     <unset>
  RenewTime:       Wed, 04 Jun 2025 15:33:04 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 04 Jun 2025 15:32:33 +0000   Wed, 04 Jun 2025 15:24:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 04 Jun 2025 15:32:33 +0000   Wed, 04 Jun 2025 15:24:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 04 Jun 2025 15:32:33 +0000   Wed, 04 Jun 2025 15:24:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 04 Jun 2025 15:32:33 +0000   Wed, 04 Jun 2025 15:24:04 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.4
  Hostname:    minikube-m03
Capacity:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24031576Ki
  pods:               110
Allocatable:
  cpu:                16
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             24031576Ki
  pods:               110
System Info:
  Machine ID:                 5bd2bde1729d4bf89f8799833f4ec30c
  System UUID:                5bd2bde1729d4bf89f8799833f4ec30c
  Boot ID:                    eb32f73a-3d0c-4284-9f48-f94d8ab88d17
  Kernel Version:             5.15.153.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.2.0/24
PodCIDRs:                     10.244.2.0/24
Non-terminated Pods:          (5 in total)
  Namespace                   Name                                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                 ------------  ----------  ---------------  -------------  ---
  default                     frontend-5798bc6fdb-gpnsj            0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m35s
  default                     mongodb-1                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m11s
  default                     streamingservice-6d6b7cbcc8-ct62x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m30s
  kube-system                 kindnet-pqpkg                        100m (0%)     100m (0%)   50Mi (0%)        50Mi (0%)      9m1s
  kube-system                 kube-proxy-hbkch                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m1s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests   Limits
  --------           --------   ------
  cpu                100m (0%)  100m (0%)
  memory             50Mi (0%)  50Mi (0%)
  ephemeral-storage  0 (0%)     0 (0%)
  hugepages-1Gi      0 (0%)     0 (0%)
  hugepages-2Mi      0 (0%)     0 (0%)
Events:
  Type     Reason                             Age                  From             Message
  ----     ------                             ----                 ----             -------
  Normal   Starting                           9m                   kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  9m2s                 kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           9m2s                 kubelet          Starting kubelet.
  Warning  CgroupV1                           9m2s                 kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            9m2s                 kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            9m1s (x2 over 9m2s)  kubelet          Node minikube-m03 status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              9m1s (x2 over 9m2s)  kubelet          Node minikube-m03 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               9m1s (x2 over 9m2s)  kubelet          Node minikube-m03 status is now: NodeHasSufficientPID
  Normal   NodeReady                          9m1s                 kubelet          Node minikube-m03 status is now: NodeReady
  Normal   RegisteredNode                     8m59s                node-controller  Node minikube-m03 event: Registered Node minikube-m03 in Controller


==> dmesg <==
[Jun 4 15:08] PCI: Fatal: No config space access function found
[  +0.009372] PCI: System does not support PCI
[  +0.117185] kvm: already loaded the other module
[  +0.394593] FS-Cache: Duplicate cookie detected
[  +0.000399] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000565] FS-Cache: O-cookie d=000000009b03dd69{9P.session} n=00000000ba9a53e7
[  +0.000497] FS-Cache: O-key=[10] '34323934393337333531'
[  +0.000593] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000453] FS-Cache: N-cookie d=000000009b03dd69{9P.session} n=000000009285a818
[  +0.000719] FS-Cache: N-key=[10] '34323934393337333531'
[  +0.002270] FS-Cache: Duplicate cookie detected
[  +0.000361] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000527] FS-Cache: O-cookie d=000000009b03dd69{9P.session} n=00000000ba9a53e7
[  +0.000544] FS-Cache: O-key=[10] '34323934393337333531'
[  +0.000316] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000622] FS-Cache: N-cookie d=000000009b03dd69{9P.session} n=000000009e6583e0
[  +0.000666] FS-Cache: N-key=[10] '34323934393337333531'
[  +0.355309] WSL (1) ERROR: ConfigApplyWindowsLibPath:2537: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000003]  failed 2
[  +0.013510] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Lisbon not found. Is the tzdata package installed?
[  +0.136418] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.005040] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000560] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000494] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000991] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.843432] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.330142] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000399] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000376] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000432] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000519] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000454] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000307] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000282] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000296] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000356] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000296] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000477] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000432] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.110291] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.195407] netlink: 'init': attribute type 4 has an invalid length.
[Jun 4 15:09] tmpfs: Unknown parameter 'noswap'
[  +4.434298] tmpfs: Unknown parameter 'noswap'
[Jun 4 15:22] tmpfs: Unknown parameter 'noswap'
[  +4.411319] tmpfs: Unknown parameter 'noswap'
[Jun 4 15:23] tmpfs: Unknown parameter 'noswap'
[Jun 4 15:24] tmpfs: Unknown parameter 'noswap'


==> etcd [ad3125d0fe95] <==
{"level":"warn","ts":"2025-06-04T15:22:53.065261Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"warn","ts":"2025-06-04T15:22:53.065341Z","caller":"etcdmain/config.go:389","msg":"--proxy-refresh-interval is deprecated in 3.5 and will be decommissioned in 3.6."}
{"level":"info","ts":"2025-06-04T15:22:53.065347Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-06-04T15:22:53.065390Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-06-04T15:22:53.065394Z","caller":"embed/etcd.go:140","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-06-04T15:22:53.065411Z","caller":"embed/etcd.go:528","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-04T15:22:53.065674Z","caller":"embed/etcd.go:148","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-06-04T15:22:53.065734Z","caller":"embed/etcd.go:323","msg":"starting an etcd server","etcd-version":"3.5.21","git-sha":"a17edfd","go-version":"go1.23.7","go-os":"linux","go-arch":"amd64","max-cpu-set":16,"max-cpu-available":16,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-06-04T15:22:53.068448Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"2.039792ms"}
{"level":"info","ts":"2025-06-04T15:22:53.071422Z","caller":"etcdserver/raft.go:506","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-06-04T15:22:53.071465Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-06-04T15:22:53.071476Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-06-04T15:22:53.071480Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-06-04T15:22:53.071484Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-06-04T15:22:53.071502Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-06-04T15:22:53.074444Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-06-04T15:22:53.076200Z","caller":"mvcc/kvstore.go:425","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-06-04T15:22:53.076249Z","caller":"etcdserver/server.go:628","msg":"restore consistentIndex","index":0}
{"level":"info","ts":"2025-06-04T15:22:53.076960Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-06-04T15:22:53.078413Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-06-04T15:22:53.078652Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-04T15:22:53.079547Z","caller":"etcdserver/server.go:759","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-06-04T15:22:53.079588Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-04T15:22:53.079601Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-04T15:22:53.079605Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-04T15:22:53.143593Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-04T15:22:53.143772Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-04T15:22:53.144368Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-04T15:22:53.144531Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-04T15:22:53.144540Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-06-04T15:22:53.159817Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-06-04T15:22:53.160055Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-06-04T15:22:53.471763Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-06-04T15:22:53.471814Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-06-04T15:22:53.471833Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-06-04T15:22:53.471845Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-06-04T15:22:53.471874Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-04T15:22:53.471880Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-06-04T15:22:53.471884Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-06-04T15:22:53.472742Z","caller":"etcdserver/server.go:2697","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-04T15:22:53.473595Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-04T15:22:53.473637Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-04T15:22:53.473851Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-04T15:22:53.473940Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-04T15:22:53.473862Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-04T15:22:53.474002Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-04T15:22:53.474017Z","caller":"etcdserver/server.go:2721","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-04T15:22:53.474021Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-04T15:22:53.474051Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-04T15:22:53.474357Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-06-04T15:22:53.474378Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-04T15:22:53.475060Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-06-04T15:32:53.647788Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":940}
{"level":"info","ts":"2025-06-04T15:32:53.652907Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":940,"took":"4.90969ms","hash":548503793,"current-db-size-bytes":3403776,"current-db-size":"3.4 MB","current-db-size-in-use-bytes":3403776,"current-db-size-in-use":"3.4 MB"}
{"level":"info","ts":"2025-06-04T15:32:53.652950Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":548503793,"revision":940,"compact-revision":-1}


==> kernel <==
 15:33:05 up 24 min,  0 users,  load average: 0.31, 0.49, 0.36
Linux minikube 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kindnet [98ad3b2d2a5e] <==
I0604 15:31:28.408337       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:31:28.408367       1 main.go:301] handling current node
I0604 15:31:28.408376       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:31:28.408379       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:31:28.408473       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:31:28.408484       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:31:38.407443       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:31:38.407466       1 main.go:301] handling current node
I0604 15:31:38.407474       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:31:38.407476       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:31:38.407563       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:31:38.407566       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:31:48.409461       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:31:48.409486       1 main.go:301] handling current node
I0604 15:31:48.409494       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:31:48.409496       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:31:48.409606       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:31:48.409617       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:31:58.405922       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:31:58.405948       1 main.go:301] handling current node
I0604 15:31:58.405957       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:31:58.405960       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:31:58.406087       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:31:58.406100       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:32:08.414409       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:32:08.414436       1 main.go:301] handling current node
I0604 15:32:08.414444       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:32:08.414446       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:32:08.414534       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:32:08.414548       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:32:18.404226       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:32:18.404267       1 main.go:301] handling current node
I0604 15:32:18.404276       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:32:18.404279       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:32:18.404354       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:32:18.404357       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:32:28.403195       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:32:28.403217       1 main.go:301] handling current node
I0604 15:32:28.403226       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:32:28.403228       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:32:28.404120       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:32:28.404144       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:32:38.405636       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:32:38.405673       1 main.go:301] handling current node
I0604 15:32:38.405682       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:32:38.405685       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:32:38.405778       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:32:38.405790       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:32:48.405355       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:32:48.405401       1 main.go:301] handling current node
I0604 15:32:48.405410       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:32:48.405412       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:32:48.405502       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:32:48.405512       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 
I0604 15:32:58.404741       1 main.go:297] Handling node with IPs: map[192.168.49.2:{}]
I0604 15:32:58.404767       1 main.go:301] handling current node
I0604 15:32:58.404776       1 main.go:297] Handling node with IPs: map[192.168.49.3:{}]
I0604 15:32:58.404779       1 main.go:324] Node minikube-m02 has CIDR [10.244.1.0/24] 
I0604 15:32:58.405421       1 main.go:297] Handling node with IPs: map[192.168.49.4:{}]
I0604 15:32:58.405436       1 main.go:324] Node minikube-m03 has CIDR [10.244.2.0/24] 


==> kube-apiserver [ce3d43e71fbb] <==
I0604 15:22:54.483100       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0604 15:22:54.483028       1 aggregator.go:169] waiting for initial CRD sync...
I0604 15:22:54.483040       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0604 15:22:54.483177       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0604 15:22:54.483182       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0604 15:22:54.483157       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0604 15:22:54.483198       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0604 15:22:54.483219       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0604 15:22:54.586025       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0604 15:22:54.586052       1 default_servicecidr_controller.go:165] Creating default ServiceCIDR with CIDRs: [10.96.0.0/12]
I0604 15:22:54.586331       1 cache.go:39] Caches are synced for LocalAvailability controller
I0604 15:22:54.586351       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0604 15:22:54.586713       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0604 15:22:54.587131       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0604 15:22:54.587169       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0604 15:22:54.587272       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0604 15:22:54.645311       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0604 15:22:54.672656       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0604 15:22:54.678227       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0604 15:22:54.678255       1 policy_source.go:240] refreshing policies
I0604 15:22:54.683689       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0604 15:22:54.683705       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0604 15:22:54.683713       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0604 15:22:54.683717       1 aggregator.go:171] initial CRD sync complete...
I0604 15:22:54.683723       1 autoregister_controller.go:144] Starting autoregister controller
I0604 15:22:54.683726       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0604 15:22:54.683729       1 cache.go:39] Caches are synced for autoregister controller
I0604 15:22:54.683752       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0604 15:22:54.684857       1 controller.go:667] quota admission added evaluator for: namespaces
I0604 15:22:54.690587       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0604 15:22:54.690634       1 default_servicecidr_controller.go:214] Setting default ServiceCIDR condition Ready to True
I0604 15:22:54.694142       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0604 15:22:54.694271       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0604 15:22:54.846901       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0604 15:22:55.487218       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0604 15:22:55.489942       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0604 15:22:55.489960       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0604 15:22:55.781185       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0604 15:22:55.808338       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0604 15:22:55.906303       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0604 15:22:55.910943       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0604 15:22:55.911611       1 controller.go:667] quota admission added evaluator for: endpoints
I0604 15:22:55.914838       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0604 15:22:56.580071       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0604 15:22:56.761237       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0604 15:22:56.768446       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0604 15:22:56.773539       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0604 15:23:01.681685       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0604 15:23:02.134025       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0604 15:23:02.137278       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0604 15:23:02.382323       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0604 15:27:34.733189       1 alloc.go:328] "allocated clusterIPs" service="default/frontend" clusterIPs={"IPv4":"10.96.104.24"}
I0604 15:27:34.734772       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0604 15:27:34.931009       1 alloc.go:328] "allocated clusterIPs" service="default/catalogservice" clusterIPs={"IPv4":"10.98.181.90"}
I0604 15:27:35.125175       1 alloc.go:328] "allocated clusterIPs" service="default/streamingservice" clusterIPs={"IPv4":"10.108.95.249"}
I0604 15:27:35.127096       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0604 15:27:35.334643       1 alloc.go:328] "allocated clusterIPs" service="default/uploadservice" clusterIPs={"IPv4":"10.109.18.24"}
I0604 15:29:39.782559       1 controller.go:667] quota admission added evaluator for: statefulsets.apps
E0604 15:30:20.935679       1 upgradeaware.go:441] Error proxying data from backend to client: websocket: close sent
I0604 15:32:54.542107       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [a9205ab0e395] <==
I0604 15:23:01.193850       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0604 15:23:01.195903       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0604 15:23:01.201538       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0604 15:23:01.212342       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0604 15:23:01.229918       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0604 15:23:01.230017       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0604 15:23:01.230602       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0604 15:23:01.230637       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0604 15:23:01.230685       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0604 15:23:01.279708       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0604 15:23:01.279733       1 shared_informer.go:357] "Caches are synced" controller="job"
I0604 15:23:01.279993       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0604 15:23:01.280005       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0604 15:23:01.280196       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0604 15:23:01.280901       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0604 15:23:01.284435       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0604 15:23:01.330110       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0604 15:23:01.331293       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0604 15:23:01.347496       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0604 15:23:01.348723       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0604 15:23:01.348834       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0604 15:23:01.348859       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0604 15:23:01.381216       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0604 15:23:01.429574       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0604 15:23:01.484100       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0604 15:23:01.484703       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0604 15:23:01.529199       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0604 15:23:01.533867       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0604 15:23:01.579355       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0604 15:23:01.579430       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0604 15:23:01.579528       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0604 15:23:01.579575       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0604 15:23:01.580274       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0604 15:23:01.580308       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0604 15:23:01.580322       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0604 15:23:01.580395       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0604 15:23:01.582742       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0604 15:23:01.606814       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0604 15:23:01.625176       1 shared_informer.go:357] "Caches are synced" controller="node"
I0604 15:23:01.625212       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0604 15:23:01.625227       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0604 15:23:01.625229       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0604 15:23:01.625232       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0604 15:23:01.629832       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0604 15:23:01.629901       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0604 15:23:01.629944       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0604 15:23:01.631132       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0604 15:23:01.995015       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0604 15:23:02.029940       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0604 15:23:02.029967       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0604 15:23:02.029973       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0604 15:23:30.606518       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m02\" does not exist"
I0604 15:23:30.619312       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube-m02" podCIDRs=["10.244.1.0/24"]
I0604 15:23:31.581257       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube-m02"
I0604 15:23:31.840068       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube-m02"
I0604 15:24:04.028438       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube-m03\" does not exist"
I0604 15:24:04.028483       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube-m02"
I0604 15:24:04.031664       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube-m03" podCIDRs=["10.244.2.0/24"]
I0604 15:24:04.343952       1 topologycache.go:237] "Can't get CPU or zone information for node" logger="endpointslice-controller" node="minikube-m02"
I0604 15:24:06.589377       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube-m03"


==> kube-proxy [f175eeace40e] <==
I0604 15:23:03.068997       1 server_linux.go:63] "Using iptables proxy"
I0604 15:23:03.379169       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0604 15:23:03.379225       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0604 15:23:03.391083       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0604 15:23:03.391126       1 server_linux.go:145] "Using iptables Proxier"
I0604 15:23:03.394173       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0604 15:23:03.398978       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0604 15:23:03.404751       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0604 15:23:03.404871       1 server.go:516] "Version info" version="v1.33.1"
I0604 15:23:03.404910       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0604 15:23:03.409571       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0604 15:23:03.415229       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0604 15:23:03.415870       1 config.go:199] "Starting service config controller"
I0604 15:23:03.415881       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0604 15:23:03.415939       1 config.go:105] "Starting endpoint slice config controller"
I0604 15:23:03.415961       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0604 15:23:03.415962       1 config.go:440] "Starting serviceCIDR config controller"
I0604 15:23:03.415970       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0604 15:23:03.415975       1 config.go:329] "Starting node config controller"
I0604 15:23:03.415988       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0604 15:23:03.516702       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0604 15:23:03.516707       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0604 15:23:03.516720       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0604 15:23:03.516729       1 shared_informer.go:357] "Caches are synced" controller="service config"


==> kube-scheduler [558aef63aa43] <==
I0604 15:22:53.552877       1 serving.go:386] Generated self-signed cert in-memory
W0604 15:22:54.587125       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0604 15:22:54.587156       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0604 15:22:54.587161       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0604 15:22:54.587165       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0604 15:22:54.599783       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0604 15:22:54.599815       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0604 15:22:54.645607       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0604 15:22:54.645636       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0604 15:22:54.645656       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0604 15:22:54.645701       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E0604 15:22:54.647305       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E0604 15:22:54.647383       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E0604 15:22:54.647402       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E0604 15:22:54.647448       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0604 15:22:54.647453       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0604 15:22:54.647462       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E0604 15:22:54.647469       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E0604 15:22:54.647469       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E0604 15:22:54.647495       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0604 15:22:54.647499       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E0604 15:22:54.647519       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E0604 15:22:54.647528       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E0604 15:22:54.647532       1 reflector.go:200] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E0604 15:22:54.647548       1 reflector.go:200] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E0604 15:22:54.647552       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E0604 15:22:54.648123       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E0604 15:22:55.463309       1 reflector.go:200] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E0604 15:22:55.484423       1 reflector.go:200] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E0604 15:22:55.526586       1 reflector.go:200] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E0604 15:22:55.589052       1 reflector.go:200] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
I0604 15:22:56.246120       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874070    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874078    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874085    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874092    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/78e1292e1d47cc7d09b2c6f5826fa624-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"78e1292e1d47cc7d09b2c6f5826fa624\") " pod="kube-system/kube-apiserver-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874098    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874104    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/3924ef3609584191d8d09190210d2d78-etcd-certs\") pod \"etcd-minikube\" (UID: \"3924ef3609584191d8d09190210d2d78\") " pod="kube-system/etcd-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874111    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874118    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874143    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874159    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874167    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 04 15:22:56 minikube kubelet[2653]: I0604 15:22:56.874174    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/0378f173c980f85a71d36305bacb0ad1-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"0378f173c980f85a71d36305bacb0ad1\") " pod="kube-system/kube-controller-manager-minikube"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.668283    2653 apiserver.go:52] "Watching apiserver"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.673253    2653 desired_state_of_world_populator.go:158] "Finished populating initial desired state of world"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.752775    2653 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.752828    2653 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.752951    2653 kubelet.go:3309] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Jun 04 15:22:57 minikube kubelet[2653]: E0604 15:22:57.759885    2653 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Jun 04 15:22:57 minikube kubelet[2653]: E0604 15:22:57.759929    2653 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Jun 04 15:22:57 minikube kubelet[2653]: E0604 15:22:57.759876    2653 kubelet.go:3311] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.775323    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.7753118570000002 podStartE2EDuration="1.775311857s" podCreationTimestamp="2025-06-04 15:22:56 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-04 15:22:57.775278378 +0000 UTC m=+1.147495368" watchObservedRunningTime="2025-06-04 15:22:57.775311857 +0000 UTC m=+1.147528847"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.786544    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.7865325589999999 podStartE2EDuration="1.786532559s" podCreationTimestamp="2025-06-04 15:22:56 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-04 15:22:57.781461388 +0000 UTC m=+1.153678380" watchObservedRunningTime="2025-06-04 15:22:57.786532559 +0000 UTC m=+1.158749551"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.861856    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.861846782 podStartE2EDuration="1.861846782s" podCreationTimestamp="2025-06-04 15:22:56 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-04 15:22:57.786700901 +0000 UTC m=+1.158917893" watchObservedRunningTime="2025-06-04 15:22:57.861846782 +0000 UTC m=+1.234063774"
Jun 04 15:22:57 minikube kubelet[2653]: I0604 15:22:57.868423    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.868412583 podStartE2EDuration="1.868412583s" podCreationTimestamp="2025-06-04 15:22:56 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-04 15:22:57.861938581 +0000 UTC m=+1.234155571" watchObservedRunningTime="2025-06-04 15:22:57.868412583 +0000 UTC m=+1.240629576"
Jun 04 15:23:01 minikube kubelet[2653]: I0604 15:23:01.601690    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/1c52890b-0da2-4a54-90fe-7df174da04fd-tmp\") pod \"storage-provisioner\" (UID: \"1c52890b-0da2-4a54-90fe-7df174da04fd\") " pod="kube-system/storage-provisioner"
Jun 04 15:23:01 minikube kubelet[2653]: I0604 15:23:01.601724    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-xlq6t\" (UniqueName: \"kubernetes.io/projected/1c52890b-0da2-4a54-90fe-7df174da04fd-kube-api-access-xlq6t\") pod \"storage-provisioner\" (UID: \"1c52890b-0da2-4a54-90fe-7df174da04fd\") " pod="kube-system/storage-provisioner"
Jun 04 15:23:01 minikube kubelet[2653]: E0604 15:23:01.706635    2653 projected.go:289] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Jun 04 15:23:01 minikube kubelet[2653]: E0604 15:23:01.706722    2653 projected.go:194] Error preparing data for projected volume kube-api-access-xlq6t for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Jun 04 15:23:01 minikube kubelet[2653]: E0604 15:23:01.706853    2653 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/1c52890b-0da2-4a54-90fe-7df174da04fd-kube-api-access-xlq6t podName:1c52890b-0da2-4a54-90fe-7df174da04fd nodeName:}" failed. No retries permitted until 2025-06-04 15:23:02.206833418 +0000 UTC m=+5.579050409 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-xlq6t" (UniqueName: "kubernetes.io/projected/1c52890b-0da2-4a54-90fe-7df174da04fd-kube-api-access-xlq6t") pod "storage-provisioner" (UID: "1c52890b-0da2-4a54-90fe-7df174da04fd") : configmap "kube-root-ca.crt" not found
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406730    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/689d3db4-8b48-4552-a40d-acef0c671a04-kube-proxy\") pod \"kube-proxy-2wrr6\" (UID: \"689d3db4-8b48-4552-a40d-acef0c671a04\") " pod="kube-system/kube-proxy-2wrr6"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406781    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/689d3db4-8b48-4552-a40d-acef0c671a04-xtables-lock\") pod \"kube-proxy-2wrr6\" (UID: \"689d3db4-8b48-4552-a40d-acef0c671a04\") " pod="kube-system/kube-proxy-2wrr6"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406796    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-zlcr4\" (UniqueName: \"kubernetes.io/projected/689d3db4-8b48-4552-a40d-acef0c671a04-kube-api-access-zlcr4\") pod \"kube-proxy-2wrr6\" (UID: \"689d3db4-8b48-4552-a40d-acef0c671a04\") " pod="kube-system/kube-proxy-2wrr6"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406826    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"cni-cfg\" (UniqueName: \"kubernetes.io/host-path/c1c8c49f-a15f-48e1-82a4-29b752c003d3-cni-cfg\") pod \"kindnet-c9zlh\" (UID: \"c1c8c49f-a15f-48e1-82a4-29b752c003d3\") " pod="kube-system/kindnet-c9zlh"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406860    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/689d3db4-8b48-4552-a40d-acef0c671a04-lib-modules\") pod \"kube-proxy-2wrr6\" (UID: \"689d3db4-8b48-4552-a40d-acef0c671a04\") " pod="kube-system/kube-proxy-2wrr6"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406878    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/c1c8c49f-a15f-48e1-82a4-29b752c003d3-xtables-lock\") pod \"kindnet-c9zlh\" (UID: \"c1c8c49f-a15f-48e1-82a4-29b752c003d3\") " pod="kube-system/kindnet-c9zlh"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406888    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-7fzwj\" (UniqueName: \"kubernetes.io/projected/c1c8c49f-a15f-48e1-82a4-29b752c003d3-kube-api-access-7fzwj\") pod \"kindnet-c9zlh\" (UID: \"c1c8c49f-a15f-48e1-82a4-29b752c003d3\") " pod="kube-system/kindnet-c9zlh"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.406900    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/c1c8c49f-a15f-48e1-82a4-29b752c003d3-lib-modules\") pod \"kindnet-c9zlh\" (UID: \"c1c8c49f-a15f-48e1-82a4-29b752c003d3\") " pod="kube-system/kindnet-c9zlh"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.507983    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/489f71dd-01c0-4bf9-82df-eedbf03abfda-config-volume\") pod \"coredns-674b8bbfcf-45jnf\" (UID: \"489f71dd-01c0-4bf9-82df-eedbf03abfda\") " pod="kube-system/coredns-674b8bbfcf-45jnf"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.508022    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-lwj49\" (UniqueName: \"kubernetes.io/projected/489f71dd-01c0-4bf9-82df-eedbf03abfda-kube-api-access-lwj49\") pod \"coredns-674b8bbfcf-45jnf\" (UID: \"489f71dd-01c0-4bf9-82df-eedbf03abfda\") " pod="kube-system/coredns-674b8bbfcf-45jnf"
Jun 04 15:23:02 minikube kubelet[2653]: I0604 15:23:02.863142    2653 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="228a83e43192f86224172c277abaf42b23eb1106fcc2755d94e56f8b26c802dc"
Jun 04 15:23:03 minikube kubelet[2653]: E0604 15:23:03.066182    2653 log.go:32] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = [failed to set up sandbox container \"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\" network for pod \"coredns-674b8bbfcf-45jnf\": networkPlugin cni failed to set up pod \"coredns-674b8bbfcf-45jnf_kube-system\" network: plugin type=\"loopback\" failed (add): missing network name:, failed to clean up sandbox container \"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\" network for pod \"coredns-674b8bbfcf-45jnf\": networkPlugin cni failed to teardown pod \"coredns-674b8bbfcf-45jnf_kube-system\" network: plugin type=\"loopback\" failed (delete): missing network name]"
Jun 04 15:23:03 minikube kubelet[2653]: E0604 15:23:03.066241    2653 kuberuntime_sandbox.go:70] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = [failed to set up sandbox container \"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\" network for pod \"coredns-674b8bbfcf-45jnf\": networkPlugin cni failed to set up pod \"coredns-674b8bbfcf-45jnf_kube-system\" network: plugin type=\"loopback\" failed (add): missing network name:, failed to clean up sandbox container \"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\" network for pod \"coredns-674b8bbfcf-45jnf\": networkPlugin cni failed to teardown pod \"coredns-674b8bbfcf-45jnf_kube-system\" network: plugin type=\"loopback\" failed (delete): missing network name]" pod="kube-system/coredns-674b8bbfcf-45jnf"
Jun 04 15:23:03 minikube kubelet[2653]: E0604 15:23:03.066264    2653 kuberuntime_manager.go:1252] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = [failed to set up sandbox container \"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\" network for pod \"coredns-674b8bbfcf-45jnf\": networkPlugin cni failed to set up pod \"coredns-674b8bbfcf-45jnf_kube-system\" network: plugin type=\"loopback\" failed (add): missing network name:, failed to clean up sandbox container \"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\" network for pod \"coredns-674b8bbfcf-45jnf\": networkPlugin cni failed to teardown pod \"coredns-674b8bbfcf-45jnf_kube-system\" network: plugin type=\"loopback\" failed (delete): missing network name]" pod="kube-system/coredns-674b8bbfcf-45jnf"
Jun 04 15:23:03 minikube kubelet[2653]: E0604 15:23:03.066311    2653 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"coredns-674b8bbfcf-45jnf_kube-system(489f71dd-01c0-4bf9-82df-eedbf03abfda)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"coredns-674b8bbfcf-45jnf_kube-system(489f71dd-01c0-4bf9-82df-eedbf03abfda)\\\": rpc error: code = Unknown desc = [failed to set up sandbox container \\\"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\\\" network for pod \\\"coredns-674b8bbfcf-45jnf\\\": networkPlugin cni failed to set up pod \\\"coredns-674b8bbfcf-45jnf_kube-system\\\" network: plugin type=\\\"loopback\\\" failed (add): missing network name:, failed to clean up sandbox container \\\"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4\\\" network for pod \\\"coredns-674b8bbfcf-45jnf\\\": networkPlugin cni failed to teardown pod \\\"coredns-674b8bbfcf-45jnf_kube-system\\\" network: plugin type=\\\"loopback\\\" failed (delete): missing network name]\"" pod="kube-system/coredns-674b8bbfcf-45jnf" podUID="489f71dd-01c0-4bf9-82df-eedbf03abfda"
Jun 04 15:23:03 minikube kubelet[2653]: I0604 15:23:03.070306    2653 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="23f2b08a99d3551c1ef4ccf121aaede55d5e331356d7114d1b8c32039ca1640f"
Jun 04 15:23:03 minikube kubelet[2653]: I0604 15:23:03.083445    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=5.083430548 podStartE2EDuration="5.083430548s" podCreationTimestamp="2025-06-04 15:22:58 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-04 15:23:03.083342843 +0000 UTC m=+6.455559837" watchObservedRunningTime="2025-06-04 15:23:03.083430548 +0000 UTC m=+6.455647542"
Jun 04 15:23:04 minikube kubelet[2653]: I0604 15:23:04.089412    2653 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4"
Jun 04 15:23:04 minikube kubelet[2653]: E0604 15:23:04.096034    2653 log.go:32] "StopPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \"coredns-674b8bbfcf-45jnf_kube-system\" network: plugin type=\"loopback\" failed (delete): missing network name" podSandboxID="420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4"
Jun 04 15:23:04 minikube kubelet[2653]: E0604 15:23:04.096088    2653 kuberuntime_manager.go:1586] "Failed to stop sandbox" podSandboxID={"Type":"docker","ID":"420b84383906ed9c6e33dfc877c5e036a104ce64bfd9cd091ab75379c6075ef4"}
Jun 04 15:23:04 minikube kubelet[2653]: E0604 15:23:04.096144    2653 kuberuntime_manager.go:1161] "killPodWithSyncResult failed" err="failed to \"KillPodSandbox\" for \"489f71dd-01c0-4bf9-82df-eedbf03abfda\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"coredns-674b8bbfcf-45jnf_kube-system\\\" network: plugin type=\\\"loopback\\\" failed (delete): missing network name\""
Jun 04 15:23:04 minikube kubelet[2653]: E0604 15:23:04.096175    2653 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"489f71dd-01c0-4bf9-82df-eedbf03abfda\" with KillPodSandboxError: \"rpc error: code = Unknown desc = networkPlugin cni failed to teardown pod \\\"coredns-674b8bbfcf-45jnf_kube-system\\\" network: plugin type=\\\"loopback\\\" failed (delete): missing network name\"" pod="kube-system/coredns-674b8bbfcf-45jnf" podUID="489f71dd-01c0-4bf9-82df-eedbf03abfda"
Jun 04 15:23:04 minikube kubelet[2653]: I0604 15:23:04.481352    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-2wrr6" podStartSLOduration=2.481338859 podStartE2EDuration="2.481338859s" podCreationTimestamp="2025-06-04 15:23:02 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-04 15:23:04.091772836 +0000 UTC m=+7.463989827" watchObservedRunningTime="2025-06-04 15:23:04.481338859 +0000 UTC m=+7.853555850"
Jun 04 15:23:07 minikube kubelet[2653]: I0604 15:23:07.104647    2653 kuberuntime_manager.go:1746] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Jun 04 15:23:07 minikube kubelet[2653]: I0604 15:23:07.105437    2653 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Jun 04 15:23:09 minikube kubelet[2653]: I0604 15:23:09.175661    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kindnet-c9zlh" podStartSLOduration=2.345631906 podStartE2EDuration="7.175651267s" podCreationTimestamp="2025-06-04 15:23:02 +0000 UTC" firstStartedPulling="2025-06-04 15:23:03.073322248 +0000 UTC m=+6.445539238" lastFinishedPulling="2025-06-04 15:23:07.903341607 +0000 UTC m=+11.275558599" observedRunningTime="2025-06-04 15:23:09.175556154 +0000 UTC m=+12.549517873" watchObservedRunningTime="2025-06-04 15:23:09.175651267 +0000 UTC m=+12.549612986"
Jun 04 15:23:20 minikube kubelet[2653]: I0604 15:23:20.224908    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-674b8bbfcf-45jnf" podStartSLOduration=18.224897105 podStartE2EDuration="18.224897105s" podCreationTimestamp="2025-06-04 15:23:02 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-06-04 15:23:20.224727653 +0000 UTC m=+23.598689373" watchObservedRunningTime="2025-06-04 15:23:20.224897105 +0000 UTC m=+23.598858822"
Jun 04 15:23:24 minikube kubelet[2653]: I0604 15:23:24.233711    2653 scope.go:117] "RemoveContainer" containerID="4274e39c9aade159db6975faa389ead1097872aa4d70631ed9880a1aeca59e17"
Jun 04 15:30:10 minikube kubelet[2653]: I0604 15:30:10.148775    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"pvc-c101521d-726a-418a-bbfa-49bae3cc30ae\" (UniqueName: \"kubernetes.io/host-path/bb73cf31-6c53-4ba0-98d2-8ba336d584f6-pvc-c101521d-726a-418a-bbfa-49bae3cc30ae\") pod \"mongodb-2\" (UID: \"bb73cf31-6c53-4ba0-98d2-8ba336d584f6\") " pod="default/mongodb-2"
Jun 04 15:30:10 minikube kubelet[2653]: I0604 15:30:10.148816    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-rlk5c\" (UniqueName: \"kubernetes.io/projected/bb73cf31-6c53-4ba0-98d2-8ba336d584f6-kube-api-access-rlk5c\") pod \"mongodb-2\" (UID: \"bb73cf31-6c53-4ba0-98d2-8ba336d584f6\") " pod="default/mongodb-2"
Jun 04 15:30:23 minikube kubelet[2653]: I0604 15:30:23.755816    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongodb-2" podStartSLOduration=0.801336322 podStartE2EDuration="13.755805201s" podCreationTimestamp="2025-06-04 15:30:10 +0000 UTC" firstStartedPulling="2025-06-04 15:30:10.447290794 +0000 UTC m=+433.852149434" lastFinishedPulling="2025-06-04 15:30:23.401759669 +0000 UTC m=+446.806618313" observedRunningTime="2025-06-04 15:30:23.755661412 +0000 UTC m=+447.160520078" watchObservedRunningTime="2025-06-04 15:30:23.755805201 +0000 UTC m=+447.160663851"

